{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl a Blog Post with Firecrawl’s Python SDK (Markdown/HTML/JSON)\n",
    "\n",
    "This notebook crawls a single URL with Firecrawl, persists multiple output formats (Markdown/HTML/JSON), and shows how to access common `Document` fields.\n",
    "\n",
    "**Target URL:**\n",
    "- https://blog1.neuralengineer.org/openai-moderation-api-multimodal-llm-with-omni-moderation-latest-text-image-63b42d5f57a7\n",
    "\n",
    "> Note: Make sure you have permission to crawl the target site and that you follow its terms/robots policies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Requirements:\n",
    "- Python 3.9+\n",
    "- A Firecrawl API key available as `FIRECRAWL_API_KEY` (loaded from `.env`)\n",
    "\n",
    "Install dependencies:\n",
    "- `pip install firecrawl python-dotenv beautifulsoup4 markdownify`\n",
    "- (optional) `pip install dicttoxml` for XML export\n",
    "\n",
    "Create a `.env` file in your project root:\n",
    "\n",
    "```bash\n",
    "FIRECRAWL_API_KEY=\"your_api_key_here\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: firecrawl in /Users/pi/miniconda3/lib/python3.13/site-packages (4.12.0)\n",
      "Requirement already satisfied: python-dotenv in /Users/pi/miniconda3/lib/python3.13/site-packages (1.1.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/pi/miniconda3/lib/python3.13/site-packages (4.14.3)\n",
      "Requirement already satisfied: markdownify in /Users/pi/miniconda3/lib/python3.13/site-packages (1.2.2)\n",
      "Requirement already satisfied: dicttoxml in /Users/pi/miniconda3/lib/python3.13/site-packages (1.7.16)\n",
      "Requirement already satisfied: requests in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (2.32.4)\n",
      "Requirement already satisfied: httpx in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (0.28.1)\n",
      "Requirement already satisfied: websockets in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (15.0.1)\n",
      "Requirement already satisfied: nest-asyncio in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (1.6.0)\n",
      "Requirement already satisfied: pydantic>=2.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (2.11.7)\n",
      "Requirement already satisfied: aiohttp in /Users/pi/miniconda3/lib/python3.13/site-packages (from firecrawl) (3.13.3)\n",
      "Requirement already satisfied: soupsieve>=1.6.1 in /Users/pi/miniconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.8.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Users/pi/miniconda3/lib/python3.13/site-packages (from markdownify) (1.17.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from pydantic>=2.0->firecrawl) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/pi/miniconda3/lib/python3.13/site-packages (from pydantic>=2.0->firecrawl) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from pydantic>=2.0->firecrawl) (0.4.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from aiohttp->firecrawl) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/pi/miniconda3/lib/python3.13/site-packages (from yarl<2.0,>=1.17.0->aiohttp->firecrawl) (3.7)\n",
      "Requirement already satisfied: anyio in /Users/pi/miniconda3/lib/python3.13/site-packages (from httpx->firecrawl) (4.11.0)\n",
      "Requirement already satisfied: certifi in /Users/pi/miniconda3/lib/python3.13/site-packages (from httpx->firecrawl) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/pi/miniconda3/lib/python3.13/site-packages (from httpx->firecrawl) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/pi/miniconda3/lib/python3.13/site-packages (from httpcore==1.*->httpx->firecrawl) (0.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/pi/miniconda3/lib/python3.13/site-packages (from anyio->httpx->firecrawl) (1.3.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/pi/miniconda3/lib/python3.13/site-packages (from requests->firecrawl) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pi/miniconda3/lib/python3.13/site-packages (from requests->firecrawl) (2.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Optional: install packages (uncomment if needed)\n",
    "%pip install firecrawl python-dotenv beautifulsoup4 markdownify dicttoxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from firecrawl import Firecrawl\n",
    "\n",
    "URL = \"https://blog1.neuralengineer.org/openai-moderation-api-multimodal-llm-with-omni-moderation-latest-text-image-63b42d5f57a7\"\n",
    "\n",
    "load_dotenv(\".env\")  # reads .env into environment\n",
    "\n",
    "api_key = os.environ.get(\"FIRECRAWL_API_KEY\")\n",
    "if not api_key:\n",
    "    raise RuntimeError(\"Missing FIRECRAWL_API_KEY in environment (create .env or export it)\")\n",
    "\n",
    "app = Firecrawl(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl a Specific URL\n",
    "\n",
    "Request multiple output formats in one call. `only_main_content=True` is a good first pass to reduce boilerplate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.scrape(\n",
    "    URL,\n",
    "    formats=[\"markdown\", \"html\", \"links\", \"images\"],\n",
    "    only_main_content=True,\n",
    ")\n",
    "\n",
    "type(result), result.model_dump(exclude_none=True).keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-processing: Save Only the Primary Article Content\n",
    "\n",
    "Even with `only_main_content=True`, post-processing the returned HTML can help you reliably isolate the article element across sites.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534cd0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from markdownify import markdownify as md\n",
    "import html as html_lib\n",
    "import json\n",
    "import re\n",
    "\n",
    "def extract_primary_html(html: str) -> str:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    for tag in soup.select(\"script, style, noscript, header, footer, nav, aside\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    primary = soup.find(\"article\") or soup.find(\"main\") or soup.body\n",
    "    return str(primary) if primary else html\n",
    "\n",
    "def _get_meta(soup: BeautifulSoup, *, name: str | None = None, prop: str | None = None) -> str | None:\n",
    "    attrs: dict[str, str] = {}\n",
    "    if name:\n",
    "        attrs[\"name\"] = name\n",
    "    if prop:\n",
    "        attrs[\"property\"] = prop\n",
    "    tag = soup.find(\"meta\", attrs=attrs)\n",
    "    return tag.get(\"content\") if tag else None\n",
    "\n",
    "def extract_metadata(page_html: str) -> dict:\n",
    "    soup = BeautifulSoup(page_html, \"html.parser\")\n",
    "    meta: dict[str, object] = {\n",
    "        \"author\": _get_meta(soup, name=\"author\") or _get_meta(soup, prop=\"article:author\"),\n",
    "        \"published_date\": _get_meta(soup, prop=\"article:published_time\"),\n",
    "        \"thumbnail_image\": _get_meta(soup, prop=\"og:image\"),\n",
    "        \"featured\": None,\n",
    "        \"reading_time_minutes\": None,\n",
    "    }\n",
    "\n",
    "    for script in soup.find_all(\"script\", attrs={\"type\": \"application/ld+json\"}):\n",
    "        try:\n",
    "            data = json.loads(script.get_text(strip=True) or \"null\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        items = data if isinstance(data, list) else [data]\n",
    "        for item in items:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            t = item.get(\"@type\")\n",
    "            if t in {\"Article\", \"BlogPosting\", \"NewsArticle\"}:\n",
    "                author = item.get(\"author\")\n",
    "                if isinstance(author, dict):\n",
    "                    meta[\"author\"] = meta[\"author\"] or author.get(\"name\")\n",
    "                meta[\"published_date\"] = meta[\"published_date\"] or item.get(\"datePublished\")\n",
    "                img = item.get(\"image\")\n",
    "                if isinstance(img, str):\n",
    "                    meta[\"thumbnail_image\"] = meta[\"thumbnail_image\"] or img\n",
    "                elif isinstance(img, list) and img and isinstance(img[0], str):\n",
    "                    meta[\"thumbnail_image\"] = meta[\"thumbnail_image\"] or img[0]\n",
    "\n",
    "    text_head = soup.get_text(\" \", strip=True)[:5000]\n",
    "    m = re.search(r\"(\\d+)\\s*min\\s*read\", text_head, re.IGNORECASE)\n",
    "    meta[\"reading_time_minutes\"] = int(m.group(1)) if m else None\n",
    "    meta[\"featured\"] = True if re.search(r\"\\bFeatured\\b\", text_head) else None\n",
    "\n",
    "    time_tag = soup.find(\"time\")\n",
    "    if time_tag:\n",
    "        meta[\"published_date\"] = meta[\"published_date\"] or time_tag.get(\"datetime\") or time_tag.get_text(\" \", strip=True)\n",
    "\n",
    "    if not meta[\"author\"]:\n",
    "        byline = soup.find(\"a\", href=re.compile(r\"byline\", re.IGNORECASE))\n",
    "        if byline:\n",
    "            meta[\"author\"] = byline.get_text(\" \", strip=True) or None\n",
    "    if not meta[\"thumbnail_image\"]:\n",
    "        img = soup.find(\"img\", src=re.compile(r\"^https?://\"))\n",
    "        if img:\n",
    "            meta[\"thumbnail_image\"] = img.get(\"src\")\n",
    "    return meta\n",
    "\n",
    "def clean_primary_html(primary_html: str) -> tuple[dict, str]:\n",
    "    soup = BeautifulSoup(primary_html, \"html.parser\")\n",
    "    container = soup.find(\"article\") or soup.find(\"main\") or soup\n",
    "\n",
    "    top_children = [c for c in container.children if getattr(c, \"name\", None)]\n",
    "\n",
    "    def remove_subscribe_popup(root) -> None:\n",
    "        pattern = re.compile(r\"stories in\\s+your\\s+inbox|join medium for free\", re.IGNORECASE)\n",
    "        for tag in list(root.find_all([\"div\", \"section\", \"form\"], recursive=True)):\n",
    "            text = tag.get_text(\" \", strip=True)\n",
    "            if not text:\n",
    "                continue\n",
    "            if pattern.search(text) and \"subscribe\" in text.lower():\n",
    "                tag.decompose()\n",
    "\n",
    "    remove_subscribe_popup(container)\n",
    "\n",
    "    if len(top_children) == 1 and len(top_children[0].find_all(\"p\")) >= 5:\n",
    "        container = top_children[0]\n",
    "\n",
    "    title_tag = container.find(\"h1\")\n",
    "    title = title_tag.get_text(\" \", strip=True) if title_tag else None\n",
    "    if title_tag:\n",
    "        title_tag.decompose()\n",
    "\n",
    "    for tag in container.select(\"header\"):\n",
    "        tag.decompose()\n",
    "\n",
    "    def is_boilerplate(text: str) -> bool:\n",
    "        t = text.strip()\n",
    "        if not t:\n",
    "            return True\n",
    "        if re.search(r\"\\\\bFollow\\\\b|\\\\bListen\\\\b|\\\\bShare\\\\b\", t):\n",
    "            return True\n",
    "        if re.search(r\"\\\\bFeatured\\\\b\", t):\n",
    "            return True\n",
    "        if re.search(r\"\\\\d+\\\\s*min\\\\s*read\", t, re.IGNORECASE):\n",
    "            return True\n",
    "        if re.search(r\"Press enter or click to view image\", t, re.IGNORECASE):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    first_content_p = None\n",
    "    for p in container.find_all(\"p\"):\n",
    "        t = p.get_text(\" \", strip=True)\n",
    "        if len(t) >= 200 and not is_boilerplate(t):\n",
    "            first_content_p = p\n",
    "            break\n",
    "\n",
    "    if first_content_p is not None:\n",
    "        node = first_content_p\n",
    "        while node is not None and node is not container:\n",
    "            for sib in list(getattr(node, \"previous_siblings\", [])):\n",
    "                if getattr(sib, \"name\", None):\n",
    "                    sib.decompose()\n",
    "            node = node.parent\n",
    "\n",
    "    return {\"title\": title}, str(container)\n",
    "\n",
    "def to_front_matter(d: dict) -> str:\n",
    "    def esc(v: object) -> str:\n",
    "        if v is None:\n",
    "            return \"null\"\n",
    "        if isinstance(v, bool):\n",
    "            return \"true\" if v else \"false\"\n",
    "        if isinstance(v, (int, float)):\n",
    "            return str(v)\n",
    "        return json.dumps(str(v))\n",
    "    keys = [\"title\", \"author\", \"published_date\", \"reading_time_minutes\", \"thumbnail_image\", \"featured\", \"source_url\"]\n",
    "    lines = [\"---\"] + [f\"{k}: {esc(d.get(k))}\" for k in keys] + [\"---\", \"\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_html_document(meta: dict, body_html: str) -> str:\n",
    "    title = html_lib.escape(str(meta.get(\"title\") or \"\"))\n",
    "    author = html_lib.escape(str(meta.get(\"author\") or \"\"))\n",
    "    published = html_lib.escape(str(meta.get(\"published_date\") or \"\"))\n",
    "    image = html_lib.escape(str(meta.get(\"thumbnail_image\") or \"\"))\n",
    "    read_mins = html_lib.escape(str(meta.get(\"reading_time_minutes\") or \"\"))\n",
    "\n",
    "    head_lines = [\n",
    "        '<meta charset=\"utf-8\">',\n",
    "        f'<title>{title}</title>' if title else '',\n",
    "        f'<meta name=\"author\" content=\"{author}\">' if author else '',\n",
    "        f'<meta name=\"reading_time_minutes\" content=\"{read_mins}\">' if read_mins else '',\n",
    "        f'<meta property=\"article:published_time\" content=\"{published}\">' if published else '',\n",
    "        f'<meta property=\"og:image\" content=\"{image}\">' if image else '',\n",
    "    ]\n",
    "    head = \"\".join([line for line in head_lines if line]).strip()\n",
    "\n",
    "    meta_entries = []\n",
    "    if title:\n",
    "        meta_entries.append(f\"<h1>{title}</h1>\")\n",
    "    if author:\n",
    "        meta_entries.append(f\"<div><strong>Author:</strong> {author}</div>\")\n",
    "    if read_mins:\n",
    "        meta_entries.append(f\"<div><strong>Reading time:</strong> {read_mins} min</div>\")\n",
    "    if published:\n",
    "        meta_entries.append(f\"<div><strong>Published:</strong> {published}</div>\")\n",
    "\n",
    "\n",
    "    meta_block = \"\"\n",
    "    if meta_entries:\n",
    "        meta_block = f\"<div class=\\\"article-meta\\\">{''.join(meta_entries)}</div>\"\n",
    "\n",
    "    body_content = f\"{meta_block}{body_html}\"\n",
    "    return f\"<!doctype html><html><head>{head}</head><body>{body_content}</body></html>\"\n",
    "\n",
    "primary_html = extract_primary_html(result.html or \"\")\n",
    "page_meta = extract_metadata(result.html or \"\")\n",
    "primary_meta, cleaned_primary_html = clean_primary_html(primary_html)\n",
    "\n",
    "doc_meta = result.metadata.model_dump(exclude_none=True) if getattr(result, \"metadata\", None) else {}\n",
    "\n",
    "def first(*vals: object) -> object:\n",
    "    for v in vals:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, str) and not v.strip():\n",
    "            continue\n",
    "        return v\n",
    "    return None\n",
    "\n",
    "merged = dict(page_meta)\n",
    "merged[\"author\"] = first(doc_meta.get(\"author\"), page_meta.get(\"author\"))\n",
    "merged[\"published_date\"] = first(doc_meta.get(\"published_time\"), doc_meta.get(\"article:published_time\"), page_meta.get(\"published_date\"))\n",
    "merged[\"thumbnail_image\"] = first(\n",
    "    doc_meta.get(\"og_image\"),\n",
    "    doc_meta.get(\"og:image\"),\n",
    "    doc_meta.get(\"twitter:image:src\"),\n",
    "    page_meta.get(\"thumbnail_image\"),\n",
    ")\n",
    "\n",
    "rt = first(doc_meta.get(\"twitter:data1\"), doc_meta.get(\"twitter:data2\"))\n",
    "if isinstance(rt, str):\n",
    "    m_rt = re.search(r\"(\\d+)\\s*min\", rt, re.IGNORECASE)\n",
    "    if m_rt:\n",
    "        merged[\"reading_time_minutes\"] = int(m_rt.group(1))\n",
    "\n",
    "metadata = {**merged, **primary_meta, \"source_url\": URL}\n",
    "\n",
    "primary_md = md(primary_html, heading_style=\"ATX\")\n",
    "clean_primary_md = md(cleaned_primary_html, heading_style=\"ATX\")\n",
    "primary_text = BeautifulSoup(primary_html, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "clean_primary_text = BeautifulSoup(cleaned_primary_html, \"html.parser\").get_text(\"\\n\", strip=True)\n",
    "\n",
    "{\n",
    "    \"title\": metadata.get(\"title\"),\n",
    "    \"author\": metadata.get(\"author\"),\n",
    "    \"published_date\": metadata.get(\"published_date\"),\n",
    "    \"reading_time_minutes\": metadata.get(\"reading_time_minutes\"),\n",
    "    \"thumbnail_image\": metadata.get(\"thumbnail_image\"),\n",
    "    \"featured\": metadata.get(\"featured\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting Output in Multiple Formats\n",
    "\n",
    "Write outputs to `out/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "Path(\"out\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result.json is not None:\n",
    "    with open(\"out/page.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        json.dump(result.json, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"out/page.html\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(result.html or \"\")\n",
    "\n",
    "with open(\"out/page.md\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(result.markdown or \"\")\n",
    "\n",
    "with open(\"out/primary.md\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(to_front_matter(metadata) + clean_primary_md)\n",
    "\n",
    "with open(\"out/primary.html\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(build_html_document(metadata, cleaned_primary_html))\n",
    "\n",
    "with open(\"out/primary.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(clean_primary_text)\n",
    "\n",
    "with open(\"out/metadata.json\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(metadata, fp, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Wrote out/page.* and out/primary.*\" + (\" (+ out/page.json)\" if result.json is not None else \"\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Structured JSON\n",
    "\n",
    "Firecrawl’s JSON output is schema-driven. The snippet below shows the expected `formats` shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'OpenAI Moderation API: multimodal LLM with `omni-moderation-latest` (text + image)', 'summary': \"This article discusses the OpenAI Moderation API's new multimodal capabilities, enabling classification of both text and images in content moderation, alongside practical usage guidelines and comparisons with older models.\"}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class PageSchema(BaseModel):\n",
    "    title: str | None = None\n",
    "    summary: str | None = None\n",
    "\n",
    "structured = app.scrape(\n",
    "    URL,\n",
    "    formats=[\n",
    "        \"markdown\",\n",
    "        {\"type\": \"json\", \"prompt\": \"Extract a title and short summary.\", \"schema\": PageSchema.model_json_schema()},\n",
    "    ],\n",
    ")\n",
    "print(structured.json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Save as XML\n",
    "\n",
    "Requires `dicttoxml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dicttoxml import dicttoxml\n",
    "#\n",
    "# if result.json is None:\n",
    "#     raise RuntimeError(\"No JSON payload available; include 'json' in formats\")\n",
    "#\n",
    "# xml_bytes = dicttoxml(result.json, custom_root=\"page\", attr_type=False)\n",
    "# with open(\"out/page.xml\", \"wb\") as fp:\n",
    "#     fp.write(xml_bytes)\n",
    "#\n",
    "# print(\"Wrote out/page.xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Useful Fields\n",
    "\n",
    "Firecrawl returns a typed `Document`. These fields are often the most useful for downstream processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result.links or []), len(result.images or [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Extractions\n",
    "\n",
    "For site-level work, use a crawl job and poll (or configure a webhook). Keep this commented for the single-page example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job = app.crawl(\n",
    "#     \"https://blog1.neuralengineer.org/\",\n",
    "#     include_paths=[\"/openai-moderation-api-multimodal-llm-with-omni-moderation-latest-text-image-63b42d5f57a7\"],\n",
    "#     limit=1,\n",
    "# )\n",
    "# job\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "- https://docs.firecrawl.dev/introduction\n",
    "- https://pypi.org/project/firecrawl/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
