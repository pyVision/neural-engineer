{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e7461ef",
   "metadata": {},
   "source": [
    "# Advanced Caching Techniques with aiocache and Redis\n",
    "\n",
    "In modern applications, effective caching strategies can dramatically improve performance. The `aiocache` library stands out as a powerful asynchronous caching solution for Python, particularly when paired with Redis. This article focuses on advanced decorator patterns that solve three common challenges:\n",
    "\n",
    "1. Bypassing cache when needed, even with valid TTL\n",
    "2. Implementing dynamic TTL based on content or context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f07208",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71561f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install aiocache redis aiohttp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f04acd",
   "metadata": {},
   "source": [
    "## Introduction to aiocache\n",
    "\n",
    "`aiocache` is an asynchronous caching library for Python that provides a common interface for different cache backends. It was designed with asyncio in mind and works particularly well in asynchronous applications.\n",
    "\n",
    "Let's begin with some basic setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a2c308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiocache import Cache, RedisCache\n",
    "from aiocache.serializers import JsonSerializer\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "import inspect\n",
    "import functools\n",
    "from datetime import datetime\n",
    "from typing import Callable, Any, Optional, Union\n",
    "\n",
    "# Configure Redis cache\n",
    "cache = RedisCache(\n",
    "    endpoint=\"localhost\",\n",
    "    port=6379,\n",
    "    namespace=\"app\",\n",
    "    password=\"asdf\",  # Change or remove this based on your Redis configuration\n",
    "    serializer=JsonSerializer()\n",
    ")\n",
    "\n",
    "# Helper function to simulate fetching data from an external source\n",
    "async def fetch_data_from_source(key=None):\n",
    "    \"\"\"Simulate fetching data from a slow external source\"\"\"\n",
    "    await asyncio.sleep(1)  # Simulate network delay\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"data\": f\"Data for {key}\" if key else \"Generic data\",\n",
    "        \"source\": \"External API\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74b7a1",
   "metadata": {},
   "source": [
    "## Beyond Basic Caching: Building an Enhanced Decorator\n",
    "\n",
    "The standard `aiocache.cached` decorator is useful, but let's create an enhanced version that allows us to:\n",
    "1. Bypass the cache on demand\n",
    "2. Set dynamic TTL values based on the result\n",
    "3. Share cached objects between functions\n",
    "4. Track cache statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d297cb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aiocache.base import SENTINEL\n",
    "from aiocache.decorators import cached as original_cached\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class enhanced_cached(original_cached):\n",
    "    \"\"\"\n",
    "    Enhanced version of aiocache's cached decorator, adding these features:\n",
    "    \n",
    "    Bypass cache using a parameter (even when TTL hasn't expired)\n",
    "    Cache invalidation methods\n",
    "    TTL override parameter for runtime flexibility and  Asynchronous TTL updates on cache hits\n",
    "    Cache statistics tracking\n",
    "    Dynamic TTL calculation based on the function result\n",
    "\n",
    "\n",
    " \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        ttl=SENTINEL,\n",
    "        ttl_func=None,\n",
    "        key=None,\n",
    "        namespace=None,\n",
    "        key_builder=None,\n",
    "        skip_cache_func=lambda x: False,\n",
    "        cache=Cache.MEMORY,\n",
    "        serializer=None,\n",
    "        plugins=None,\n",
    "        alias=None,\n",
    "        noself=False,\n",
    "        cache_key_prefix=None,\n",
    "        shared_context=None,\n",
    "        track_stats=False,\n",
    "        update_ttl_on_hit=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            ttl=ttl,\n",
    "            key=key,\n",
    "            namespace=namespace,\n",
    "            key_builder=key_builder,\n",
    "            skip_cache_func=skip_cache_func,\n",
    "            cache=cache,\n",
    "            serializer=serializer,\n",
    "            plugins=plugins,\n",
    "            alias=alias,\n",
    "            noself=noself,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.ttl_func = ttl_func\n",
    "        self.cache_key_prefix = cache_key_prefix\n",
    "        self.shared_context = shared_context\n",
    "        self.track_stats = track_stats\n",
    "        self.stats = {\"hits\": 0, \"misses\": 0, \"bypasses\": 0}\n",
    "        self.update_ttl_on_hit = update_ttl_on_hit\n",
    "\n",
    "    def __call__(self, f):\n",
    "        wrapped = super().__call__(f)\n",
    "        \n",
    "        # Add additional metadata and methods to the wrapped function\n",
    "        wrapped.original_func = f\n",
    "        wrapped.cache_stats = self.stats\n",
    "        \n",
    "        # Add a method to explicitly invalidate the cache for specific args\n",
    "        async def invalidate(*args, **kwargs):\n",
    "            \"\"\"Explicitly invalidate cache for these function arguments\"\"\"\n",
    "            key = self.get_cache_key(f, args, kwargs)\n",
    "            await self.cache.delete(key)\n",
    "            return True\n",
    "        \n",
    "        wrapped.invalidate = invalidate\n",
    "        \n",
    "        # Add a method to refresh the cache for specific args\n",
    "        async def refresh(*args, **kwargs):\n",
    "            \"\"\"Force refresh of the cache for these function arguments\"\"\"\n",
    "            key = self.get_cache_key(f, args, kwargs)\n",
    "            # Execute function and update cache\n",
    "            result = await f(*args, **kwargs)\n",
    "            ttl = self._calculate_ttl(result)\n",
    "            await self.cache.set(key, result, ttl=ttl)\n",
    "            return result\n",
    "            \n",
    "        wrapped.refresh = refresh\n",
    "        \n",
    "        return wrapped\n",
    "\n",
    "    def get_cache_key(self, f, args, kwargs):\n",
    "        \"\"\"Enhanced key generation with support for sharing contexts\"\"\"\n",
    "        if self.key:\n",
    "            base_key = self.key\n",
    "        elif self.key_builder:\n",
    "            base_key = self.key_builder(f, *args, **kwargs)\n",
    "        else:\n",
    "            base_key = self._key_from_args(f, args, kwargs)\n",
    "        \n",
    "        # If we have a shared context, use that instead of the function name\n",
    "        if self.shared_context:\n",
    "            parts = base_key.split(\":\", 1)\n",
    "            if len(parts) > 1:\n",
    "                # Replace function name with shared context\n",
    "                base_key = f\"{self.shared_context}:{parts[1]}\" \n",
    "            else:\n",
    "                base_key = f\"{self.shared_context}:{base_key}\"\n",
    "        \n",
    "        # Apply prefix if specified\n",
    "        if self.cache_key_prefix:\n",
    "            base_key = f\"{self.cache_key_prefix}:{base_key}\"\n",
    "            \n",
    "        return base_key\n",
    "\n",
    "    async def decorator(\n",
    "        self, f, *args, bypass_cache=False, force_refresh=False, \n",
    "        cache_read=True, cache_write=True, \n",
    "        aiocache_wait_for_write=True, ttl=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Enhanced decorator with bypass_cache, force_refresh, and TTL override options\n",
    "        \n",
    "        Args:\n",
    "            ttl: Optional TTL override to use instead of the default\n",
    "                - On cache hit: Updates existing TTL in background (if update_ttl_on_hit=True)\n",
    "                - On cache miss: Uses this TTL when storing the result\n",
    "        \"\"\"\n",
    "        # Extract ttl from kwargs if provided (before removing it)\n",
    "        runtime_ttl = kwargs.pop('ttl', None) if ttl is None else ttl\n",
    "        \n",
    "        if runtime_ttl is not None:\n",
    "            ttl=runtime_ttl\n",
    "            \n",
    "        #print(\"AAAA\", runtime_ttl)\n",
    "        # Generate cache key\n",
    "        key = self.get_cache_key(f, args, kwargs)\n",
    "        \n",
    "        # Handle cache bypass\n",
    "        if bypass_cache or force_refresh:\n",
    "            if self.track_stats:\n",
    "                self.stats[\"bypasses\"] += 1\n",
    "            # Skip cache lookup, directly call function\n",
    "            result = await f(*args, **kwargs)\n",
    "            \n",
    "            # Update cache if needed (unless skip_cache_func says otherwise)\n",
    "            if not self.skip_cache_func(result) and cache_write and (force_refresh or bypass_cache):\n",
    "                # Use provided TTL or calculate default\n",
    "                if ttl is not None:\n",
    "                    effective_ttl = ttl  \n",
    "                else:\n",
    "                    effective_ttl = self._calculate_ttl(result)\n",
    "                    \n",
    "                if aiocache_wait_for_write:\n",
    "                    await self.set_in_cache(key, result, effective_ttl)\n",
    "                else:\n",
    "                    asyncio.create_task(self.set_in_cache(key, result, effective_ttl))\n",
    "                    \n",
    "            return result\n",
    "        \n",
    "        # Normal path - check cache first\n",
    "        if cache_read:\n",
    "            value = await self.get_from_cache(key)\n",
    "            if value is not None:\n",
    "                # Cache hit\n",
    "                if self.track_stats:\n",
    "                    self.stats[\"hits\"] += 1\n",
    "                \n",
    "                # On cache hit, update TTL in background if requested\n",
    "                if runtime_ttl is not None :\n",
    "                    # Don't await - run asynchronously to avoid blocking\n",
    "                    asyncio.create_task(self._update_ttl(key, runtime_ttl))\n",
    "                \n",
    "                return value\n",
    "            \n",
    "        # Cache miss or skipped read\n",
    "        if self.track_stats:\n",
    "            self.stats[\"misses\"] += 1\n",
    "            \n",
    "        result = await f(*args, **kwargs)\n",
    "        \n",
    "        # Skip caching if needed\n",
    "        if self.skip_cache_func(result):\n",
    "            return result\n",
    "            \n",
    "        if cache_write:\n",
    "            effective_ttl = self._calculate_ttl(result)\n",
    "            if aiocache_wait_for_write:\n",
    "                await self.set_in_cache(key, result, effective_ttl)\n",
    "            else:\n",
    "                asyncio.create_task(self.set_in_cache(key, result, effective_ttl))\n",
    "                \n",
    "        return result\n",
    "        \n",
    "    async def _update_ttl(self, key, ttl):\n",
    "        \"\"\"Update TTL for an existing cache entry without blocking\"\"\"\n",
    "        try:\n",
    "            # Update expiration time for the key\n",
    "            await self.cache.expire(key, ttl)\n",
    "        except Exception:\n",
    "            logger.exception(\"Couldn't update TTL for key %s\", key)\n",
    "        \n",
    "    def _calculate_ttl(self, result):\n",
    "        \"\"\"Calculate TTL dynamically if ttl_func is provided\"\"\"\n",
    "        if self.ttl_func and callable(self.ttl_func):\n",
    "            return self.ttl_func(result)\n",
    "        return self.ttl\n",
    "        \n",
    "    async def set_in_cache(self, key, value, ttl=None):\n",
    "        \"\"\"Enhanced cache setter with dynamic TTL support\"\"\"\n",
    "        try:\n",
    "            ttl = ttl if ttl is not None else self.ttl\n",
    "            await self.cache.set(key, value, ttl=ttl)\n",
    "        except Exception:\n",
    "            logger.exception(\"Couldn't set %s in key %s, unexpected error\", value, key)\n",
    "\n",
    "# Helper function for creating shared cache keys between functions\n",
    "def shared_key_builder(shared_prefix):\n",
    "    \"\"\"\n",
    "    Create a key builder that uses a shared prefix instead of function name\n",
    "    \n",
    "    :param shared_prefix: The shared prefix to use instead of function name\n",
    "    \"\"\"\n",
    "    def key_builder(func, *args, **kwargs):\n",
    "        # Convert args to strings, filtering out non-serializable objects\n",
    "        args_str = \":\".join(str(arg) for arg in args if not callable(arg))\n",
    "        kwargs_str = \":\".join(f\"{k}={v}\" for k, v in kwargs.items())\n",
    "        return f\"{shared_prefix}:{args_str}:{kwargs_str}\"\n",
    "    \n",
    "    return key_builder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbaf0a8",
   "metadata": {},
   "source": [
    "## 1. Bypassing Cache Even When TTL Hasn't Expired\n",
    "\n",
    "Let's explore three different approaches for bypassing the cache using our enhanced decorator:\n",
    "1. Using the `bypass_cache=True` parameter\n",
    "2. Using explicit cache invalidation with `.invalidate()`\n",
    "3. Using custom cache key control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a027e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Basic usage with bypass_cache\n",
    "@enhanced_cached(    \n",
    "    ttl=10,\n",
    "    cache=Cache.REDIS,\n",
    "    track_stats=True,\n",
    "    serializer=JsonSerializer(),\n",
    "    namespace=\"n1\",\n",
    "    endpoint=\"localhost\",\n",
    "    port=6379,\n",
    "    password=\"asdf\",\n",
    "    db=0,\n",
    "    pool_max_size=10                 \n",
    "                 )  # 30 seconds TTL for demo\n",
    "async def get_user_data(user_id):\n",
    "    \"\"\"Get user data with caching\"\"\"\n",
    "    print(f\"Fetching data for user {user_id} from source...\")\n",
    "    await asyncio.sleep(0.5)  # Simulate API call\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"name\": f\"User {user_id}\",\n",
    "        \"last_updated\": datetime.now().isoformat()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9848ef05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Testing bypass_cache parameter:\n",
      "First call (should be cache miss):\n",
      "Fetching data for user 101 from source...\n",
      "  Timestamp: 2025-05-18T10:26:49.437604\n",
      "\n",
      "Second call (should be cache hit):\n",
      "  Timestamp: 2025-05-18T10:26:49.437604\n",
      "  Same data? True\n",
      "\n",
      "Third call with bypass_cache=True:\n",
      "Fetching data for user 101 from source...\n",
      "  Timestamp: 2025-05-18T10:26:49.942818\n",
      "  Fresh data? True\n",
      "\n",
      "2. Testing explicit invalidation:\n",
      "Invalidating cache...\n",
      "\n",
      "Call after invalidation (should be cache miss):\n",
      "Fetching data for user 101 from source...\n",
      "  Timestamp: 2025-05-18T10:26:50.446097\n",
      "  Fresh data? True\n",
      "\n",
      "Cache Statistics:\n",
      "  {'hits': 1, 'misses': 2, 'bypasses': 1}\n"
     ]
    }
   ],
   "source": [
    "async def test_cache_bypass():\n",
    "    print(\"1. Testing bypass_cache parameter:\")\n",
    "    \n",
    "    # First call - should be cache miss\n",
    "    print(\"First call (should be cache miss):\")\n",
    "    user1 = await get_user_data(101)\n",
    "    print(f\"  Timestamp: {user1['last_updated']}\")\n",
    "    \n",
    "    # Second call - should be cache hit with same timestamp\n",
    "    print(\"\\nSecond call (should be cache hit):\")\n",
    "    user2 = await get_user_data(101)\n",
    "    print(f\"  Timestamp: {user2['last_updated']}\")\n",
    "    print(f\"  Same data? {user1['last_updated'] == user2['last_updated']}\")\n",
    "    \n",
    "    # Third call with bypass_cache=True - should be fresh data\n",
    "    print(\"\\nThird call with bypass_cache=True:\")\n",
    "    user3 = await get_user_data(101, bypass_cache=True)\n",
    "    print(f\"  Timestamp: {user3['last_updated']}\")\n",
    "    print(f\"  Fresh data? {user1['last_updated'] != user3['last_updated']}\")\n",
    "    # Test explicit cache invalidation\n",
    "    print(\"\\n2. Testing explicit invalidation:\")\n",
    "    \n",
    "    # First invalidate the cache\n",
    "    print(\"Invalidating cache...\")\n",
    "    await get_user_data.invalidate(101)\n",
    "    \n",
    "    # This should be a cache miss\n",
    "    print(\"\\nCall after invalidation (should be cache miss):\")\n",
    "    user4 = await get_user_data(101)\n",
    "    print(f\"  Timestamp: {user4['last_updated']}\")\n",
    "    print(f\"  Fresh data? {user3['last_updated'] != user4['last_updated']}\")\n",
    "    \n",
    "    # Display cache statistics\n",
    "    print(\"\\nCache Statistics:\")\n",
    "    print(f\"  {get_user_data.cache_stats}\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "await test_cache_bypass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c43c1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Testing versioned cache keys:\n",
      "First call with v1:\n",
      "AAAA None\n",
      "Fetching product 201 (version v1) from source...\n",
      "  Version: v1\n",
      "  Timestamp: 2025-05-19T18:53:28.390971\n",
      "\n",
      "Second call with v1:\n",
      "AAAA None\n",
      "  Version: v1\n",
      "  Timestamp: 2025-05-19T18:53:28.390971\n",
      "  Cache hit? True\n",
      "\n",
      "Call with v2:\n",
      "AAAA None\n",
      "Fetching product 201 (version v2) from source...\n",
      "  Version: v2\n",
      "  Timestamp: 2025-05-19T18:53:28.896589\n",
      "  Fresh data? True\n"
     ]
    }
   ],
   "source": [
    "# Example with custom key builder for more control over caching\n",
    "def versioned_key_builder(func, *args, **kwargs):\n",
    "    \"\"\"Build a cache key that includes version information\"\"\"\n",
    "    base_key = func.__name__\n",
    "    \n",
    "    # Extract primary arg (assume it's first positional arg)\n",
    "    entity_id = args[0] if args else \"unknown\"\n",
    "    \n",
    "    # Extract version from kwargs or use default\n",
    "    version = kwargs.get('version', 'v1')\n",
    "    \n",
    "    return f\"{base_key}:{entity_id}:ver{version}\"\n",
    "\n",
    "\n",
    "\n",
    "@enhanced_cached(    \n",
    "    key_builder=versioned_key_builder,\n",
    "    ttl=10,\n",
    "    cache=Cache.REDIS,\n",
    "    track_stats=True,\n",
    "    serializer=JsonSerializer(),\n",
    "    namespace=\"n1\",\n",
    "    endpoint=\"localhost\",\n",
    "    port=6379,\n",
    "    password=\"asdf\",\n",
    "    db=0,\n",
    "    pool_max_size=10                 \n",
    "                 ) \n",
    "async def get_product_data(product_id, version='v1'):\n",
    "    \"\"\"Get product data with versioned caching\"\"\"\n",
    "    print(f\"Fetching product {product_id} (version {version}) from source...\")\n",
    "    await asyncio.sleep(0.5)  # Simulate API call\n",
    "    return {\n",
    "        \"product_id\": product_id,\n",
    "        \"name\": f\"Product {product_id}\",\n",
    "        \"version\": version,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "async def test_versioned_cache():\n",
    "    print(\"\\n3. Testing versioned cache keys:\")\n",
    "    \n",
    "    # First call with v1 - should be cache miss\n",
    "    print(\"First call with v1:\")\n",
    "    product1 = await get_product_data(201, version='v1')\n",
    "    print(f\"  Version: {product1['version']}\")\n",
    "    print(f\"  Timestamp: {product1['timestamp']}\")\n",
    "    \n",
    "    # Second call with v1 - should be cache hit\n",
    "    print(\"\\nSecond call with v1:\")\n",
    "    product2 = await get_product_data(201, version='v1')\n",
    "    print(f\"  Version: {product2['version']}\")\n",
    "    print(f\"  Timestamp: {product2['timestamp']}\")\n",
    "    print(f\"  Cache hit? {product1['timestamp'] == product2['timestamp']}\")\n",
    "    \n",
    "    # Call with v2 - should be cache miss (different key)\n",
    "    print(\"\\nCall with v2:\")\n",
    "    product3 = await get_product_data(201, version='v2')\n",
    "    print(f\"  Version: {product3['version']}\")\n",
    "    print(f\"  Timestamp: {product3['timestamp']}\")\n",
    "    print(f\"  Fresh data? {product1['timestamp'] != product3['timestamp']}\")\n",
    "\n",
    "# Run the test\n",
    "await test_versioned_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366ddf3",
   "metadata": {},
   "source": [
    "## 2. Implementing Dynamic TTL Values\n",
    "\n",
    "Now let's explore how our enhanced decorator can implement dynamic TTL strategies:\n",
    "1. Using a TTL function based on result content\n",
    "2. Different TTL values for different types of objects\n",
    "3. Scaling TTL based on result complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2459077b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Dynamic TTL:\n",
      "\n",
      "1. Testing subscription-based TTL:\n",
      "Regular user:\n",
      "AAAA None\n",
      "  Plan: basic\n",
      "  TTL: 20 seconds\n",
      "\n",
      "Premium user:\n",
      "AAAA None\n",
      "  Plan: premium\n",
      "  TTL: 60 seconds\n",
      "\n",
      "2. Testing complexity-based TTL:\n",
      "Simple catalog:\n",
      "AAAA None\n",
      "  Products: 9\n",
      "  Calculated TTL: 42 seconds\n",
      "\n",
      "Detailed catalog:\n",
      "AAAA None\n",
      "  Products: 3\n",
      "  Calculated TTL: 60 seconds\n",
      "\n",
      "Comparing TTLs:\n",
      "  Simple catalog TTL: 42 seconds\n",
      "  Detailed catalog TTL: 60 seconds\n",
      "  Difference: 18 seconds\n",
      "\n",
      "3. Testing dynamically updateable TTL:\n",
      "AAAA None\n",
      "  Product ID: {'product_id': 333, 'name': 'Product 333', 'price': 73, 'in_stock': True, 'timestamp': '2025-05-20T17:42:29.736748'}\n",
      "AAAA 60\n",
      "  Product ID: {'product_id': 333, 'name': 'Product 333', 'price': 73, 'in_stock': True, 'timestamp': '2025-05-20T17:42:29.736748'}\n",
      "AAAA None\n",
      "  Product ID: {'product_id': 333, 'name': 'Product 333', 'price': 73, 'in_stock': True, 'timestamp': '2025-05-20T17:42:29.736748'}\n"
     ]
    }
   ],
   "source": [
    "# Example with dynamic TTL based on result content\n",
    "@enhanced_cached(\n",
    "    ttl=10,\n",
    "    cache=Cache.REDIS,\n",
    "    track_stats=True,\n",
    "    serializer=JsonSerializer(),\n",
    "    namespace=\"n1\",\n",
    "    endpoint=\"localhost\",\n",
    "    port=6379,\n",
    "    password=\"asdf\",\n",
    "    db=0,\n",
    "    pool_max_size=10, \n",
    "    ttl_func=lambda result: 60 if result.get('is_premium') else 20  # 60s for premium, 20s for regular\n",
    ")\n",
    "async def get_user_subscription(user_id):\n",
    "    \"\"\"Get user subscription with dynamic TTL based on subscription type\"\"\"\n",
    "    print(f\"Fetching subscription for user {user_id}...\")\n",
    "    await asyncio.sleep(0.5)  # Simulate API call\n",
    "    is_premium = user_id % 3 == 0  # Every 3rd user is premium\n",
    "    return {\n",
    "        \"user_id\": user_id,\n",
    "        \"is_premium\": is_premium,\n",
    "        \"plan\": \"premium\" if is_premium else \"basic\",\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "# Example with TTL based on result complexity\n",
    "def complexity_based_ttl(result):\n",
    "    \"\"\"TTL scales based on data complexity/size\"\"\"\n",
    "    if not result:\n",
    "        return 10  # Default low TTL for empty results\n",
    "        \n",
    "    # Estimate complexity by serialized size\n",
    "    size = len(json.dumps(result))\n",
    "    \n",
    "    # Scale TTL: larger results get cached longer\n",
    "    # Between 20-60 seconds based on size\n",
    "    return max(20, min(60, 20 + (size // 20)))\n",
    "\n",
    "@enhanced_cached(    ttl=10,\n",
    "    cache=Cache.REDIS,\n",
    "    track_stats=True,\n",
    "    serializer=JsonSerializer(),\n",
    "    namespace=\"n1\",\n",
    "    endpoint=\"localhost\",\n",
    "    port=6379,\n",
    "    password=\"asdf\",\n",
    "    db=0,\n",
    "    pool_max_size=10    \n",
    "    , ttl_func=complexity_based_ttl)\n",
    "async def get_product_catalog(category, include_details=False):\n",
    "    \"\"\"Get product catalog with dynamically scaled TTL\"\"\"\n",
    "    print(f\"Fetching catalog for {category} (details: {include_details})...\")\n",
    "    await asyncio.sleep(0.8)  # Simulate API call\n",
    "    \n",
    "    # Generate variable number of products\n",
    "    product_count = (hash(category) % 10) + 2\n",
    "    \n",
    "    products = []\n",
    "    for i in range(1, product_count + 1):\n",
    "        product = {\n",
    "            \"id\": i,\n",
    "            \"name\": f\"{category} Product {i}\",\n",
    "        }\n",
    "        \n",
    "        # Add more details to increase complexity/size\n",
    "        if include_details:\n",
    "            product.update({\n",
    "                \"description\": f\"Detailed description for {category} Product {i}\",\n",
    "                \"specifications\": {\n",
    "                    \"weight\": f\"{i * 0.5} kg\",\n",
    "                    \"dimensions\": f\"{i * 5} x {i * 3} x {i * 2} cm\",\n",
    "                    \"colors\": [\"red\", \"blue\", \"black\"] if i % 2 else [\"white\", \"grey\"],\n",
    "                },\n",
    "                \"features\": [f\"Feature {j}\" for j in range(1, (i % 5) + 3)]\n",
    "            })\n",
    "    \n",
    "        products.append(product)\n",
    "    \n",
    "    result = {\n",
    "        \"category\": category,\n",
    "        \"product_count\": len(products),\n",
    "        \"products\": products,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "async def test_dynamic_ttl():\n",
    "    print(\"\\nTesting Dynamic TTL:\")\n",
    "    \n",
    "    # Test premium vs regular subscriptions\n",
    "    print(\"\\n1. Testing subscription-based TTL:\")\n",
    "    \n",
    "    # Regular user\n",
    "    print(\"Regular user:\")\n",
    "    regular = await get_user_subscription(101)  # Non-premium\n",
    "    print(f\"  Plan: {regular['plan']}\")\n",
    "    print(f\"  TTL: 20 seconds\")\n",
    "    \n",
    "    # Premium user\n",
    "    print(\"\\nPremium user:\")\n",
    "    premium = await get_user_subscription(102)  # Premium\n",
    "    print(f\"  Plan: {premium['plan']}\")\n",
    "    print(f\"  TTL: 60 seconds\")\n",
    "    \n",
    "    # Test complexity-based TTL\n",
    "    print(\"\\n2. Testing complexity-based TTL:\")\n",
    "    \n",
    "    # Simple catalog\n",
    "    print(\"Simple catalog:\")\n",
    "    simple_catalog = await get_product_catalog(\"Books\")\n",
    "    simple_ttl = complexity_based_ttl(simple_catalog)\n",
    "    print(f\"  Products: {simple_catalog['product_count']}\")\n",
    "    print(f\"  Calculated TTL: {simple_ttl} seconds\")\n",
    "    \n",
    "    # Detailed catalog (larger/more complex)\n",
    "    print(\"\\nDetailed catalog:\")\n",
    "    detailed_catalog = await get_product_catalog(\"Electronics\", include_details=True)\n",
    "    detailed_ttl = complexity_based_ttl(detailed_catalog)\n",
    "    print(f\"  Products: {detailed_catalog['product_count']}\")\n",
    "    print(f\"  Calculated TTL: {detailed_ttl} seconds\")\n",
    "    \n",
    "    print(\"\\nComparing TTLs:\")\n",
    "    print(f\"  Simple catalog TTL: {simple_ttl} seconds\")\n",
    "    print(f\"  Detailed catalog TTL: {detailed_ttl} seconds\")\n",
    "    print(f\"  Difference: {detailed_ttl - simple_ttl} seconds\")\n",
    "\n",
    "# Example with dynamically updateable TTL after caching\n",
    "@enhanced_cached(\n",
    "        ttl=30,\n",
    "        cache=Cache.REDIS,\n",
    "        track_stats=True,\n",
    "        serializer=JsonSerializer(),\n",
    "        namespace=\"n1\",\n",
    "        endpoint=\"localhost\",\n",
    "        port=6379,\n",
    "        password=\"asdf\",\n",
    "        db=0,\n",
    "        pool_max_size=10,\n",
    "        # Default TTL is 30 seconds, but can be updated after caching\n",
    "    )\n",
    "async def get_product_data(product_id, ttl=None):\n",
    "        \"\"\"Get product data with TTL that can be dynamically updated on subsequent calls\"\"\"\n",
    "        print(f\"Fetching data for product {product_id}...\")\n",
    "        await asyncio.sleep(0.7)  # Simulate API call\n",
    "        \n",
    "        # Generate product details based on ID\n",
    "        return {\n",
    "            \"product_id\": product_id,\n",
    "            \"name\": f\"Product {product_id}\",\n",
    "            \"price\": round(10 + (product_id % 90), 2),\n",
    "            \"in_stock\": product_id % 4 != 0,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "\n",
    "# Test dynamically updating TTL\n",
    "async def test_dynamic_ttl_update():\n",
    "        print(\"\\n3. Testing dynamically updateable TTL:\")\n",
    "        \n",
    "        # First call - cache miss (uses default TTL of 30 seconds)\n",
    "        data1 = await get_product_data(333)\n",
    "        print(f\"  Product ID: {data1}\")\n",
    "        # Second call - cache hit (asynchronously updates TTL to 60 seconds)\n",
    "        data2 = await get_product_data(333, ttl=60)\n",
    "        print(f\"  Product ID: {data2}\")\n",
    "        # Third call after waiting - will still be a cache hit \n",
    "        # because TTL was extended to 60 seconds\n",
    "        await asyncio.sleep(35)  # Wait longer than the default TTL\n",
    "        data3 = await get_product_data(333)  # Still a cache hit\n",
    "        print(f\"  Product ID: {data3}\")\n",
    "\n",
    "\n",
    "# Run the test\n",
    "await test_dynamic_ttl()\n",
    "\n",
    "await test_dynamic_ttl_update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a782c3c2",
   "metadata": {},
   "source": [
    "## Practical Example: Domain Check with Smart Caching\n",
    "\n",
    "Let's implement a comprehensive example that demonstrates all these concepts working together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29de2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherService:\n",
    "    def __init__(self, cache):\n",
    "        self.cache = cache\n",
    "        \n",
    "        # Create enhanced decorators for weather data\n",
    "        self.weather_cached = enhanced_cached(\n",
    "            cache=cache,\n",
    "\n",
    "            track_stats=True\n",
    "        )\n",
    "        \n",
    " \n",
    "    \n",
    "\n",
    "    \n",
    "    async def fetch_weather_data(self, location):\n",
    "        \"\"\"Simulate fetching weather data from an API\"\"\"\n",
    "        print(f\"Fetching current weather for {location}...\")\n",
    "        await asyncio.sleep(1.2)  # Simulate API call\n",
    "        \n",
    "        # Generate simulated weather data\n",
    "        import random\n",
    "        conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Clear\"]\n",
    "        condition = random.choice(conditions)\n",
    "        \n",
    "        return {\n",
    "            \"location\": {\n",
    "                \"name\": location,\n",
    "                \"region\": \"Test Region\",\n",
    "                \"country\": \"Test Country\",\n",
    "                \"localtime\": datetime.now().isoformat()\n",
    "            },\n",
    "            \"current\": {\n",
    "                \"temp_c\": round(random.uniform(10, 30), 1),\n",
    "                \"humidity\": random.randint(30, 90),\n",
    "                \"condition\": {\n",
    "                    \"text\": condition,\n",
    "                    \"code\": random.randint(1000, 1030)\n",
    "                },\n",
    "                \"wind_kph\": round(random.uniform(0, 30), 1),\n",
    "                \"feelslike_c\": round(random.uniform(10, 32), 1)\n",
    "            },\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    async def fetch_forecast_data(self, location, days):\n",
    "        \"\"\"Simulate fetching forecast data from an API\"\"\"\n",
    "        print(f\"Fetching {days}-day forecast for {location}...\")\n",
    "        await asyncio.sleep(1.5)  # Simulate API call\n",
    "        \n",
    "        # Generate simulated forecast data\n",
    "        import random\n",
    "        conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Rainy\", \"Stormy\", \"Clear\"]\n",
    "        \n",
    "        forecast_days = []\n",
    "        start_date = datetime.now()\n",
    "        \n",
    "        for i in range(days):\n",
    "            day_date = start_date + asyncio.timedelta(days=i)\n",
    "            condition = random.choice(conditions)\n",
    "            day_forecast = {\n",
    "                \"date\": day_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"day\": {\n",
    "                    \"maxtemp_c\": round(random.uniform(15, 35), 1),\n",
    "                    \"mintemp_c\": round(random.uniform(5, 20), 1),\n",
    "                    \"condition\": {\n",
    "                        \"text\": condition,\n",
    "                        \"code\": random.randint(1000, 1030)\n",
    "                    },\n",
    "                    \"chance_of_rain\": random.randint(0, 100)\n",
    "                }\n",
    "            }\n",
    "            forecast_days.append(day_forecast)\n",
    "        \n",
    "        return {\n",
    "            \"location\": {\n",
    "                \"name\": location,\n",
    "                \"country\": \"Test Country\"\n",
    "            },\n",
    "            \"forecast\": {\n",
    "                \"forecastday\": forecast_days\n",
    "            },\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    # Decorated methods with custom parameters\n",
    "    async def get_current_weather(self, location, bypass_cache=False):\n",
    "        \"\"\"Get current weather with intelligent caching\"\"\"\n",
    "        @self.weather_cached\n",
    "        async def _get_weather(loc):\n",
    "            return await self.fetch_weather_data(loc)\n",
    "        \n",
    "        return await _get_weather(location, bypass_cache=bypass_cache)\n",
    "    \n",
    "    async def get_forecast(self, location, days=3, bypass_cache=False):\n",
    "        \"\"\"Get weather forecast with intelligent caching\"\"\"\n",
    "        # Use function-specific cache key\n",
    "        @self.forecast_cached\n",
    "        async def _get_forecast(loc, d):\n",
    "            return await self.fetch_forecast_data(loc, d)\n",
    "            \n",
    "        return await _get_forecast(location, days, bypass_cache=bypass_cache)\n",
    "    \n",
    "    async def get_weather_stats(self):\n",
    "        \"\"\"Get cache statistics\"\"\"\n",
    "        # In a real implementation, you'd get data from your cache decorators\n",
    "        weather_stats = getattr(self.weather_cached, \"stats\", {\"hits\": 0, \"misses\": 0, \"bypasses\": 0})\n",
    "        forecast_stats = getattr(self.forecast_cached, \"stats\", {\"hits\": 0, \"misses\": 0, \"bypasses\": 0})\n",
    "        \n",
    "        return {\n",
    "            \"current_weather\": weather_stats,\n",
    "            \"forecast\": forecast_stats\n",
    "        }\n",
    "\n",
    "async def test_weather_service():\n",
    "    # Create weather service\n",
    "    weather_service = WeatherService(cache)\n",
    "    \n",
    "    print(\"\\nWeather Service with Smart Caching:\")\n",
    "    \n",
    "    # Get weather for New York\n",
    "    print(\"\\n1. First call for New York weather:\")\n",
    "    ny_weather = await weather_service.get_current_weather(\"New York\")\n",
    "    print(f\"  Temperature: {ny_weather['current']['temp_c']}°C\")\n",
    "    print(f\"  Conditions: {ny_weather['current']['condition']['text']}\")\n",
    "    print(f\"  Timestamp: {ny_weather['timestamp']}\")\n",
    "    \n",
    "    # Get weather for New York again (should be cached)\n",
    "    print(\"\\n2. Second call for New York (should be cached):\")\n",
    "    ny_weather2 = await weather_service.get_current_weather(\"New York\")\n",
    "    print(f\"  Temperature: {ny_weather2['current']['temp_c']}°C\")\n",
    "    print(f\"  Conditions: {ny_weather2['current']['condition']['text']}\")\n",
    "    print(f\"  Timestamp: {ny_weather2['timestamp']}\")\n",
    "    print(f\"  Cache hit? {ny_weather['timestamp'] == ny_weather2['timestamp']}\")\n",
    "    \n",
    "    # Force refresh New York weather\n",
    "    print(\"\\n3. Forced refresh for New York weather:\")\n",
    "    ny_weather3 = await weather_service.get_current_weather(\"New York\", bypass_cache=True)\n",
    "    print(f\"  Temperature: {ny_weather3['current']['temp_c']}°C\")\n",
    "    print(f\"  Conditions: {ny_weather3['current']['condition']['text']}\")\n",
    "    print(f\"  Timestamp: {ny_weather3['timestamp']}\")\n",
    "    print(f\"  Fresh data? {ny_weather['timestamp'] != ny_weather3['timestamp']}\")\n",
    "    \n",
    "    # Get forecast for London\n",
    "    print(\"\\n4. Fetching 3-day forecast for London:\")\n",
    "    london_forecast = await weather_service.get_forecast(\"London\", days=3)\n",
    "    print(f\"  Forecast days: {len(london_forecast['forecast']['forecastday'])}\")\n",
    "    print(f\"  First day: {london_forecast['forecast']['forecastday'][0]['date']}\")\n",
    "    print(f\"  Conditions: {london_forecast['forecast']['forecastday'][0]['day']['condition']['text']}\")\n",
    "    print(f\"  Timestamp: {london_forecast['timestamp']}\")\n",
    "    \n",
    "    # Get forecast again (should be cached)\n",
    "    print(\"\\n5. Second call for London forecast (should be cached):\")\n",
    "    london_forecast2 = await weather_service.get_forecast(\"London\", days=3)\n",
    "    print(f\"  Timestamp: {london_forecast2['timestamp']}\")\n",
    "    print(f\"  Cache hit? {london_forecast['timestamp'] == london_forecast2['timestamp']}\")\n",
    "    \n",
    "    # Get cache statistics\n",
    "    print(\"\\n6. Cache Statistics:\")\n",
    "    stats = await weather_service.get_weather_stats()\n",
    "    print(f\"  Current Weather: {stats['current_weather']}\")\n",
    "    print(f\"  Forecast: {stats['forecast']}\")\n",
    "\n",
    "# Run the test\n",
    "await test_weather_service()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6301d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've demonstrated how to extend aiocache's decorators to create a powerful caching system that addresses three key challenges:\n",
    "\n",
    "1. **Bypassing Cache When Needed**\n",
    "   - Using `bypass_cache=True` parameter\n",
    "   - Implementing explicit `.invalidate()` methods\n",
    "   - Using versioned or custom cache keys\n",
    "\n",
    "2. **Dynamic TTL Implementation**\n",
    "   - Setting TTL based on result content (premium vs. regular)\n",
    "   - Adjusting TTL based on data characteristics (size/complexity)\n",
    "   - Customizing TTL based on business logic (weather stability)\n",
    "\n",
    "3. **Sharing Cached Objects Across Functions**\n",
    "   - Using identical key builders for different functions\n",
    "   - Setting shared context identifiers\n",
    "   - Creating factory functions for related function groups\n",
    "\n",
    "Our enhanced decorator provides a clean, declarative approach to caching that combines flexibility with performance. By intelligently managing cache lifetimes and sharing cached objects when appropriate, applications can achieve optimal balance between performance and data freshness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
