{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6246be9",
   "metadata": {},
   "source": [
    "## Meta Title\n",
    "Download Complete English Wikipedia knowledge base for large-scale semantic search and AI applications\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this guide, I’ll walk you through the entire process of downloading, parsing, and preparing the complete English Wikipedia knowledge base for advanced AI applications like large-scale semantic search . You’ll learn how to select the right dump files, automate downloads, handle massive datasets, and apply best practices for reliability and performance. Whether you’re building an NLP pipeline, a custom search engine, or training embeddings, this tutorial provides the technical steps and context needed for robust Wikipedia data engineering.\n",
    "\n",
    "\n",
    "# Downloading the English Wikipedia Database\n",
    "\n",
    "Wikimedia offers several types of database dumps, each tailored for different technical needs.  You can browse and download these files from the [Wikimedia Dumps](https://dumps.wikimedia.org/enwiki/latest/) page.Here’s a quick rundown:\n",
    "\n",
    "- **Pages-Articles Dumps (`enwiki-latest-pages-articles*.xml.bz2`)**  \n",
    "  *Contains the text of all Wikipedia articles, excluding talk pages, user pages, and other non-content pages. This is the go-to dump for NLP and embedding tasks.*\n",
    "\n",
    "- **Pages-Articles Multistream Dumps (`enwiki-latest-pages-articles-multistream*.xml.bz2`)**  \n",
    "  *Designed for efficient random access, these files come with an index for quick retrieval of specific articles.*\n",
    "\n",
    "- **Pages-Meta-Current Dumps (`enwiki-latest-pages-meta-current*.xml.bz2`)**  \n",
    "  *Includes the current revision of all pages, plus metadata like timestamps and contributor info.*\n",
    "\n",
    "- **Pages-Meta-History Dumps (`enwiki-latest-pages-meta-history*.xml.bz2`)**  \n",
    "  *Contains the full revision history for all pages—ideal for research on editing behavior or historical analysis.*\n",
    "\n",
    "- **Full XML Dumps (`enwiki-latest.xml.bz2`)**  \n",
    "  *All pages and complete revision history for comprehensive research.*\n",
    "\n",
    "- **Abstracts Dumps (`enwiki-latest-abstract.xml.gz`)**  \n",
    "  *Short summaries of each article for lightweight applications.*\n",
    "\n",
    "- **SQL Dumps (`*.sql.gz`)**  \n",
    "  *Database tables in SQL format for advanced analysis or custom mirrors.*\n",
    "\n",
    "- **Image, Category, Pagelinks, User, Redirect, and Other Specialized Dumps**  \n",
    "  *Each serves specific analytical or archival purposes.*\n",
    "\n",
    "\n",
    "\n",
    "# Pages Article Dump\n",
    "\n",
    "In this artcile we will look at downloading **Pages-Articles Dump** . The **Pages-Articles Dump** is the most widely used dataset for NLP and embedding tasks. The main file, `enwiki-latest-pages-articles.xml.bz2`, contains the full text of all Wikipedia articles, excluding non-content pages like talk, user, and file pages. This dump is updated regularly and is the recommended source for extracting article content for semantic search and machine learning projects.\n",
    "\n",
    "\n",
    "## Split Dumps: Handling Large Files\n",
    "\n",
    "Due to the massive size of the English Wikipedia, the pages-articles dump is often split into multiple parts for easier downloading and processing. These files are named sequentially, such as:\n",
    "\n",
    "```\n",
    "enwiki-latest-pages-articles1.xml-p1p41242.bz2\n",
    "enwiki-latest-pages-articles2.xml-p41243p151573.bz2\n",
    "enwiki-latest-pages-articles3.xml-p151574p311329.bz2\n",
    "...\n",
    "```\n",
    "\n",
    "Each split file contains a portion of the articles, with the filename indicating the page ID range (e.g., `p1p41242` means pages with IDs from 1 to 41,242). The main file, `enwiki-latest-pages-articles.xml.bz2`, may be a concatenation of these splits or a separate full dump, depending on the release.\n",
    "\n",
    "\n",
    "## Typical Dump Sizes\n",
    "\n",
    "- **Complete Dump (compressed):** ~20–25 GB (`enwiki-latest-pages-articles.xml.bz2`)\n",
    "- **Single Split Part (compressed):** ~1–2 GB (e.g., `enwiki-latest-pages-articles1.xml-p1p41242.bz2`)\n",
    "- **Complete Dump (uncompressed):** ~80–100 GB\n",
    "- **Single Split Part (uncompressed):** ~4–8 GB\n",
    "\n",
    "*Sizes vary by release and Wikipedia growth. Always check the actual file sizes on the [Wikimedia Dumps](https://dumps.wikimedia.org/enwiki/latest/) page.*\n",
    "\n",
    "Handling Wikipedia’s massive size requires smart strategies:\n",
    "\n",
    "- **File Size:** The full articles dump can exceed 20GB compressed and over 80GB uncompressed. Splitting makes downloads manageable and reduces corruption risk.\n",
    "- **Parallel Processing:** Splits allow you to process multiple chunks in parallel, speeding up parsing and embedding.\n",
    "- **Resilience:** If a download fails, you only need to re-download the affected split, not the entire dataset.\n",
    "\n",
    "\n",
    "# Programatically Downloading the Wikipedia Data\n",
    "\n",
    "To automate the process of finding and downloading the latest Wikipedia article dumps, you can use the official RSS feed and the `WikiDumpClient` class (see notebook for code). This approach ensures you always get the most recent files and can handle both split and combined dumps.\n",
    "\n",
    "## Use the RSS Feed to Find the Latest Dump\n",
    "\n",
    "Set the RSS feed URL:\n",
    "\n",
    "```python\n",
    "DEFAULT_RSS_URL = \"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2-rss.xml\"\n",
    "```\n",
    "\n",
    "The `WikiDumpClient.get_latest_dump_link_from_rss(rss_url)` method fetches and parses this RSS feed to extract the latest dump directory URL (e.g., `https://dumps.wikimedia.org/enwiki/20250920`).\n",
    "\n",
    "## Download and Parse dumpstatus.json\n",
    "\n",
    "The dumstatus.json is found at `https://dumps.wikimedia.org/enwiki/20250920/dumpstatus.json`\n",
    "\n",
    "## About `dumpstatus.json`\n",
    "\n",
    "The `dumpstatus.json` file is a machine-readable summary of the current Wikipedia dump directory.\n",
    " (e.g., [20250920](https://dumps.wikimedia.org/enwiki/20250920/)). It provides structured metadata about all files generated during the dump process, including:\n",
    "\n",
    "- **File names and URLs:** Direct download links for each dump file (e.g., split articles, combined dumps, indexes, SQL tables).\n",
    "- **Status:** Whether each file has finished processing, is in progress, or failed.\n",
    "- **File sizes:** Both compressed and uncompressed sizes.\n",
    "- **Checksums:** MD5 or SHA1 hashes for verifying file integrity.\n",
    "- **Timestamps:** When each file was started, finished, or last updated.\n",
    "- **Job metadata:** Information about the dump run, such as job names, types, and completion status.\n",
    "\n",
    "**Example snippet from `dumpstatus.json`:**\n",
    "```json\n",
    "{\n",
    "  \"jobs\": {\n",
    "    \"pages-articles\": {\n",
    "      \"files\": {\n",
    "        \"enwiki-20250920-pages-articles1.xml-p1p41242.bz2\": {\n",
    "          \"url\": \"https://dumps.wikimedia.org/enwiki/20250920/enwiki-20250920-pages-articles1.xml-p1p41242.bz2\",\n",
    "          \"size\": 123456789,\n",
    "          \"sha1\": \"abcdef123456...\",\n",
    "          \"status\": \"done\"\n",
    "        },\n",
    "        // ...more files...\n",
    "      },\n",
    "      \"status\": \"done\"\n",
    "    },\n",
    "    // ...other jobs...\n",
    "  }\n",
    "}\n",
    "```\n",
    "You can use this file to programmatically list, verify, and download the latest Wikipedia dump files for your project.\n",
    "\n",
    "## Extract File Lists for Split and Combined Dumps\n",
    "\n",
    "- To get the list of split \"pages-articles\" files, use:\n",
    "  \n",
    "  The `dumpstatus.json` file contains a `\"jobs\"` dictionary, where each key is a dump job (such as `\"pages-articles\"`). Inside each job, the `\"files\"` dictionary lists all output files for that job. For split dumps, each part (e.g., `enwiki-20250920-pages-articles1.xml-p1p41242.bz2`, `enwiki-20250920-pages-articles2.xml-p41243p151573.bz2`, etc.) appears as a separate entry. Each file entry includes metadata such as the download URL, size, checksum, and status. To extract all split \"pages-articles\" files, iterate over the `\"files\"` dictionary under the `\"articlesdump\"` job and collect the file URLs or names where the status is `\"done\"`.\n",
    "\n",
    "\n",
    "  ```python\n",
    "  split_files = client.get_articlesdump(dump_json)\n",
    "  ```\n",
    "  This method parses the `\"pages-articles\"` job in `dumpstatus.json` and returns a list of all split article dump files that are ready for download.\n",
    "\n",
    "- To get the combined file (if available), use:\n",
    "\n",
    "  The combined \"pages-articles\" file is a single, large compressed XML file that contains the entire set of Wikipedia articles in one file, rather than being split into multiple parts. This file is typically named in the format `enwiki-YYYYMMDD-pages-articles.xml.bz2` (for example, `enwiki-20250920-pages-articles.xml.bz2`). Not every dump run produces a combined file, but when available, it is listed in the `\"files\"` dictionary under the `\"articlesdumprecombines\"` job in `dumpstatus.json`.\n",
    "\n",
    "\n",
    "  ```python\n",
    "  combined_files = client.get_articlesdumpcombine(dump_json)\n",
    "  ```\n",
    "  This method will return a list (usually of length 0 or 1) containing the combined articles dump file(s) that are available and ready for download.\n",
    "\n",
    "## Example Code\n",
    "\n",
    "```python\n",
    "from wiki1 import WikiDumpClient, DEFAULT_RSS_URL\n",
    "\n",
    "client = WikiDumpClient()\n",
    "dump_json = client.download_links(DEFAULT_RSS_URL)\n",
    "split_files = client.get_articlesdump(dump_json)         # List of split files\n",
    "combined_files = client.get_articlesdumpcombine(dump_json)  # List of combined files (if available)\n",
    "```\n",
    "\n",
    "This workflow ensures you always have the latest file list for Wikipedia articles, and the code handles caching, retries, and error handling for robust automation.\n",
    "\n",
    "# Downloading Process\n",
    "\n",
    "For downloading, I recommend using PycURL for its speed, reliability, and efficient handling of large files as detailed in [High Performance File Downloads in Python with PycURL](https://medium.com/neural-engineer/high-performance-file-downloads-in-python-with-pycurl-f3adcfddfaa8). PycURL leverages libcurl’s optimized networking stack, supporting\n",
    "\n",
    "- **Streaming Downloads:** Avoid loading entire files into memory by streaming data directly to disk.\n",
    "- **Resumable Downloads:** Support for HTTP range requests allows interrupted downloads to resume, saving bandwidth and time.\n",
    "- **Progress Monitoring:** PycURL provides hooks for real-time progress updates, useful for tracking large downloads.\n",
    "- **Error Handling & Retries:** Implement robust error checking and automatic retries for transient network issues.\n",
    "- **Connection Management:** Efficiently reuse connections for multiple files, reducing overhead.\n",
    "\n",
    "The Full source code can be found at \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Downloading and embedding the entire English Wikipedia is a challenging yet incredibly rewarding project for any technical AI engineer. By leveraging the latest Wikimedia dumps, parsing structured metadata from `dumpstatus.json`, and using high-performance tools like PycURL, you can build a scalable pipeline for semantic search, knowledge extraction, and advanced NLP applications.\n",
    "\n",
    "If you found this guide helpful, consider subscribing to my newsletter for more deep dives into AI, NLP, and scalable machine learning. Share your feedback, questions, or experiences in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180cdac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install prerequisite packages for Wikipedia dump downloading and processing\n",
    "!pip install pycurl requests tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "568680ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pycurl\n",
    "import hashlib\n",
    "\n",
    "class FileDownloader:\n",
    "\tdef __init__(self, download_dir, chunk_size=8192, callback=None):\n",
    "\t\tself.download_dir = download_dir\n",
    "\t\tself.chunk_size = chunk_size\n",
    "\t\tself.callback = callback\n",
    "\n",
    "\tdef _md5sum(self, file_path):\n",
    "\t\t\"\"\"Compute md5 hash of a file asynchronously.\"\"\"\n",
    "\t\thash_md5 = hashlib.md5()\n",
    "\t\t\n",
    "\t\twith open(file_path, \"rb\") as f:\n",
    "\t\t\twhile True:\n",
    "\t\t\t\tchunk = f.read(self.chunk_size)\n",
    "\t\t\t\tif not chunk:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\thash_md5.update(chunk)\n",
    "\t\treturn hash_md5.hexdigest()\n",
    "\n",
    "\tdef _validate_file(self, file_path, expected_size=None, expected_md5=None):\n",
    "\t\tif not os.path.exists(file_path):\n",
    "\t\t\treturn False\n",
    "\t\tif expected_size is not None and os.path.getsize(file_path) != expected_size:\n",
    "\t\t\treturn False\n",
    "\t\tif expected_md5 is not None:\n",
    "\t\t\tactual_md5 = self._md5sum(file_path)\n",
    "\t\t\tif actual_md5 != expected_md5:\n",
    "\t\t\t\treturn False\n",
    "\t\treturn True\n",
    "\n",
    "\tdef download(self, url, filename, expected_size=None, expected_md5=None):\n",
    "\t\tfile_path = os.path.join(self.download_dir, filename)\n",
    "\n",
    "\t\t# Check if file already exists and is valid\n",
    "\t\tif self._validate_file(file_path, expected_size, expected_md5):\n",
    "\t\t\tprint(f\"File {file_path} already exists and is valid. Skipping download.\")\n",
    "\t\t\treturn file_path\n",
    "\n",
    "\t\tmd5 = hashlib.md5()\n",
    "\t\treceived = 0\n",
    "\n",
    "\t\t# Try to get total bytes from Content-Length header\n",
    "\t\ttotal_bytes = None\n",
    "\t\tc = pycurl.Curl()\n",
    "\t\tc.setopt(c.URL, url)\n",
    "\t\tc.setopt(c.NOBODY, 1)\n",
    "\t\tc.perform()\n",
    "\t\ttry:\n",
    "\t\t\ttotal_bytes = int(c.getinfo(pycurl.CONTENT_LENGTH_DOWNLOAD))\n",
    "\t\texcept Exception:\n",
    "\t\t\ttotal_bytes = expected_size\n",
    "\t\tc.close()\n",
    "\n",
    "\t\tdef write_callback(data):\n",
    "\t\t\tnonlocal received\n",
    "\t\t\tf.write(data)\n",
    "\t\t\tmd5.update(data)\n",
    "\t\t\treceived += len(data)\n",
    "\t\t\tif self.callback:\n",
    "\t\t\t\tself.callback(received, total_bytes)\n",
    "\n",
    "\t\twith open(file_path, 'wb') as f:\n",
    "\t\t\tc = pycurl.Curl()\n",
    "\t\t\tc.setopt(c.URL, url)\n",
    "\t\t\tc.setopt(c.WRITEFUNCTION, write_callback)\n",
    "\t\t\tc.perform()\n",
    "\t\t\tc.close()\n",
    "\n",
    "\t\t# Check file size\n",
    "\t\tif expected_size is not None and received != expected_size:\n",
    "\t\t\traise ValueError(f\"Size mismatch: expected {expected_size}, got {received}\")\n",
    "\n",
    "\t\t# Check MD5\n",
    "\t\tif expected_md5 is not None and md5.hexdigest() != expected_md5:\n",
    "\t\t\traise ValueError(\"MD5 checksum mismatch\")\n",
    "\n",
    "\t\treturn file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28598ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import logging\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"wikidump\")\n",
    "\n",
    "DEFAULT_RSS_URL = \"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2-rss.xml\"\n",
    "\n",
    "class WikiDumpClient:\n",
    "    def __init__(self, retries=3, backoff_factor=0.3, timeout=10, download_dir=\".\"):\n",
    "        self.session = self._requests_session_with_retries(retries, backoff_factor, timeout)\n",
    "        self.timeout = timeout\n",
    "        self.download_dir = download_dir\n",
    "\n",
    "    def _requests_session_with_retries(self, retries, backoff_factor, timeout):\n",
    "        session = requests.Session()\n",
    "        retry = Retry(\n",
    "            total=retries,\n",
    "            read=retries,\n",
    "            connect=retries,\n",
    "            backoff_factor=backoff_factor,\n",
    "            status_forcelist=(500, 502, 503, 504),\n",
    "            allowed_methods=[\"GET\", \"POST\"]\n",
    "        )\n",
    "        adapter = HTTPAdapter(max_retries=retry)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        session.request_timeout = timeout\n",
    "        return session\n",
    "\n",
    "    def get_json_with_logging(self, url):\n",
    "        try:\n",
    "            logger.info(f\"Requesting: {url}\")\n",
    "            url = url.replace(\"downloads.wikimedia.org\", \"dumps.wikimedia.org\")\n",
    "            resp = self.session.get(url, timeout=self.session.request_timeout)\n",
    "            logger.info(f\"Response: {resp.status_code} {resp.reason}\")\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "        except requests.RequestException as e:\n",
    "            logger.error(f\"Request failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def print_dump_files(self, dump_json):\n",
    "        if not dump_json or \"jobs\" not in dump_json:\n",
    "            logger.error(\"Invalid dumpstatus.json structure.\")\n",
    "            return\n",
    "        jobs = dump_json[\"jobs\"]\n",
    "        for key in [\"pages-articles\", \"pages-articles-multistream\"]:\n",
    "            if key in jobs:\n",
    "                logger.info(f\"\\n=== {key} ===\")\n",
    "                files = jobs[key].get(\"files\", {})\n",
    "                for fname, finfo in files.items():\n",
    "                    logger.info(f\"{fname}: {finfo.get('url', '')}\")\n",
    "\n",
    "    def get_latest_dump_link_from_rss(self, rss_url=None):\n",
    "        if not rss_url:\n",
    "            rss_url = DEFAULT_RSS_URL\n",
    "        try:\n",
    "            logger.info(f\"Requesting RSS: {rss_url}\")\n",
    "            rss_url = rss_url.replace(\"downloads.wikimedia.org\", \"dumps.wikimedia.org\")\n",
    "            resp = self.session.get(rss_url, timeout=self.session.request_timeout)\n",
    "            logger.info(f\"Response: {resp.status_code} {resp.reason}\")\n",
    "            resp.raise_for_status()\n",
    "            root = ET.fromstring(resp.content)\n",
    "            item = root.find(\".//item\")\n",
    "            if item is not None:\n",
    "                link_elem = item.find(\"link\")\n",
    "                if link_elem is not None and link_elem.text:\n",
    "                    logger.info(f\"Found dump link: {link_elem.text}\")\n",
    "                    url = link_elem.text\n",
    "                    url_no_protocol = url.split('://', 1)[-1] if '://' in url else url\n",
    "                    url_no_protocol = url_no_protocol.replace(\"download.wikimedia.org\", \"dumps.wikimedia.org\")\n",
    "                    return url_no_protocol\n",
    "            logger.error(\"Could not find dump link in RSS feed.\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to parse RSS: {e}\")\n",
    "            return None\n",
    "\n",
    "    def read_last_dump_url(self, filename):\n",
    "        if os.path.exists(filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                return f.read().strip()\n",
    "        return None\n",
    "\n",
    "    def write_last_dump_url(self, filename, url):\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(url)\n",
    "\n",
    "    def read_json_file(self, filename):\n",
    "        try:\n",
    "            with open(filename, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                logger.info(f\"Successfully read JSON from {filename}\")\n",
    "                return data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read JSON from {filename}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def download_links(self, rss_url=None, last_url_file=\"last_dump_url.txt\", json_file=\"dumpstatus.json\"):\n",
    "        \"\"\"Return cached JSON if present and up-to-date, else download and update cache if RSS URL has changed.\"\"\"\n",
    "        if not rss_url:\n",
    "            rss_url = DEFAULT_RSS_URL\n",
    "        dump_url = self.get_latest_dump_link_from_rss(rss_url)\n",
    "        last_dump_url = self.read_last_dump_url(last_url_file)\n",
    "        # If no dump_url, fallback to cached json if present\n",
    "        if not dump_url:\n",
    "            logger.warning(\"No dump_url found in RSS. Returning cached JSON if available.\")\n",
    "            return self.read_json_file(json_file)\n",
    "        # If dump_url unchanged and json_file exists, return cached json\n",
    "        if dump_url == last_dump_url and os.path.exists(json_file):\n",
    "            logger.info(\"Dump URL unchanged. Returning cached JSON.\")\n",
    "            return self.read_json_file(json_file)\n",
    "        # Otherwise, download new json, update cache, and return it\n",
    "        logger.info(\"Dump URL changed or cache missing. Downloading new dumpstatus.json.\")\n",
    "        dumpstatus_url = f\"https://{dump_url}/dumpstatus.json\"\n",
    "        dump_json = self.get_json_with_logging(dumpstatus_url)\n",
    "        if dump_json:\n",
    "            with open(json_file, 'w') as jf:\n",
    "                json.dump(dump_json, jf, indent=2)\n",
    "            self.write_last_dump_url(last_url_file, dump_url)\n",
    "            logger.info(\"Updated cache with new dumpstatus.json.\")\n",
    "            return dump_json\n",
    "        else:\n",
    "            logger.error(\"Failed to fetch new dumpstatus.json. Returning cached JSON if available.\")\n",
    "            return self.read_json_file(json_file)\n",
    "\n",
    "    def get_articlesdump(self, dump_json):\n",
    "        \"\"\"Return the main articles dump file (single file, not split).\"\"\"\n",
    "        a=[]\n",
    "        try:\n",
    "            files = dump_json['jobs']['articlesdump']['files']\n",
    "            for fname, finfo in files.items():\n",
    "                d={}\n",
    "                d=finfo\n",
    "                a.append(d)\n",
    "                #print(finfo.get('url', ''))\n",
    "                #if fname.endswith('.xml.bz2') and '-' not in fname:\n",
    "                #    return finfo\n",
    "            return a\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            logger.error(f\"Error getting articlesdump: {e}\")\n",
    "        return None\n",
    "\n",
    "    def get_articlesdumpcombine(self, dump_json):\n",
    "        \"\"\"Return the recombined articles dump file (if present).\"\"\"\n",
    "        try:\n",
    "            a=[]\n",
    "            files = dump_json['jobs']['articlesdumprecombine']['files']\n",
    "            for fname, finfo in files.items():\n",
    "                d={}\n",
    "                d=finfo\n",
    "                a.append(d)\n",
    "                #print(finfo.get('url', ''))\n",
    "            return a        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error getting articlesdumprecombine: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "    def download_articlesdump(self,  download_dir=None, chunk_size=16384, progress_callback=None):\n",
    "        \"\"\"Download all files from articlesdump using FileDownloader.\"\"\"\n",
    "        dump_json = self.download_links()\n",
    "        from pathlib import Path\n",
    "        # Import FileDownloader from the notebook's context\n",
    "\n",
    "        files = self.get_articlesdump(dump_json)\n",
    "        if not files:\n",
    "            logger.error(\"No articlesdump files found to download.\")\n",
    "            return []\n",
    "        if download_dir is None:\n",
    "            download_dir = self.download_dir\n",
    "        Path(download_dir).mkdir(parents=True, exist_ok=True)\n",
    "        downloader = FileDownloader(download_dir=download_dir, chunk_size=chunk_size, callback=progress_callback)\n",
    "        downloaded_files = []\n",
    "        for f in files:\n",
    "            url = f.get('url')\n",
    "            url = f\"https://dumps.wikimedia.org\" + url\n",
    "            if not url:\n",
    "                logger.warning(f\"No URL for file entry: {f}\")\n",
    "                continue\n",
    "            filename = url.split('/')[-1]\n",
    "            expected_size = f.get('size')\n",
    "            expected_md5 = f.get('md5')\n",
    "            try:\n",
    "                logger.info(f\"Downloading {filename} from {url}\")\n",
    "                file_path = downloader.download(url, filename, expected_size=expected_size, expected_md5=expected_md5)\n",
    "                downloaded_files.append(file_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download {filename}: {e}\")\n",
    "        return downloaded_files\n",
    "\n",
    "    def download_articlesdumpcombine(self,  download_dir=None, chunk_size=16384, progress_callback=None):\n",
    "        \"\"\"Download all files from articlesdumprecombines using FileDownloader.\"\"\"\n",
    "        dump_json = self.download_links()\n",
    "        from pathlib import Path\n",
    "\n",
    "        files = self.get_articlesdumpcombine(dump_json)\n",
    "        if not files:\n",
    "            logger.error(\"No articlesdumprecombine files found to download.\")\n",
    "            return []\n",
    "        if download_dir is None:\n",
    "            download_dir = self.download_dir\n",
    "        Path(download_dir).mkdir(parents=True, exist_ok=True)\n",
    "        downloader = FileDownloader(download_dir=download_dir, chunk_size=chunk_size, callback=progress_callback)\n",
    "        downloaded_files = []\n",
    "        for f in files:\n",
    "            url = f.get('url')\n",
    "            url = f\"https://dumps.wikimedia.org\" + url\n",
    "            if not url:\n",
    "                logger.warning(f\"No URL for file entry: {f}\")\n",
    "                continue\n",
    "            filename = url.split('/')[-1]\n",
    "            expected_size = f.get('size')\n",
    "            expected_md5 = f.get('md5')\n",
    "            try:\n",
    "                logger.info(f\"Downloading {filename} from {url}\")\n",
    "                file_path = downloader.download(url, filename, expected_size=expected_size, expected_md5=expected_md5)\n",
    "                downloaded_files.append(file_path)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Failed to download {filename}: {e}\")\n",
    "        return downloaded_files\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9128adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define a progress callback function\n",
    "client = WikiDumpClient()\n",
    "\n",
    "last_percent = {'value': -5}\n",
    "\n",
    "def progress_callback(bytes_downloaded, total_bytes):\n",
    "    percent = (bytes_downloaded / total_bytes) * 100 if total_bytes else 0\n",
    "    if percent - last_percent['value'] >= 5 or percent == 100:\n",
    "        print(f\"Downloaded {bytes_downloaded}/{total_bytes} bytes ({percent:.2f}%)\")\n",
    "        last_percent['value'] = percent\n",
    "\n",
    "# Specify a custom output directory\n",
    "output_dir = \"/tmp/wikidump\"\n",
    "\n",
    "# Download articles dump with progress callback and custom output directory\n",
    "downloaded_files = client.download_articlesdump(\n",
    "        download_dir=output_dir,\n",
    "        chunk_size=16384,\n",
    "        progress_callback=progress_callback\n",
    "    )\n",
    "print(\"Downloaded files:\", downloaded_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
