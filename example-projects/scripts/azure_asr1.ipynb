{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e804010",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "In this post, I’ll show you how to harness the power of **Azure Speech to Text** in Python to transcribe audio files and byte streams—quickly, accurately, and with advanced features like word-level timestamps, speaker diarization, and automatic language detection.\n",
    "\n",
    "Whether you’re building live meeting captioning, interactive voice agents, or large-scale audio analytics, you’ll gain practical skills and insights to make your solutions smarter and more responsive. Let’s dive in!\n",
    "\n",
    "# Why Azure Speech to Text?\n",
    "\n",
    "The Azure Speech to Text service provides the following core features:\n",
    "\n",
    "- Real-time transcription: Instant transcription with intermediate results for live audio inputs.\n",
    "- Fast transcription: Synchronous output optimized for predictable latency scenarios.\n",
    "- Batch transcription: Efficient processing for large volumes of prerecorded audio.\n",
    "- Custom speech: Models with enhanced accuracy for specific domains and conditions.\n",
    "\n",
    "This article focuses on real-time transcription \n",
    "\n",
    "# Real-Time Transcription\n",
    "\n",
    "Real-time speech-to-text technology enables the immediate conversion of spoken audio—captured from microphones or digital files—into structured text output as the audio is processed. This capability is essential for applications requiring low-latency transcription, such as live meeting captioning, interactive voice agents, and automated documentation systems.\n",
    "\n",
    "By leveraging advanced voice activity detection (VAD) and streaming recognition algorithms, real-time transcription systems can deliver intermediate results, support speaker diarization, and integrate seamlessly with engineering and AI workflows. These solutions are optimized for scenarios where rapid feedback, continuous monitoring, and integration with downstream analytics or automation pipelines are required, ensuring both accuracy and responsiveness in dynamic environments.\n",
    "\n",
    "# Setting Up Azure Speech SDK in Python\n",
    "\n",
    "First, install the Azure Speech SDK:\n",
    "\n",
    "```shell\n",
    "pip install azure-cognitiveservices-speech\n",
    "```\n",
    "\n",
    "The Azure AI Speech SDK provides two primary classes for speech transcription:\n",
    "\n",
    "- `speechsdk.transcription.ConversationTranscriber`: Supports advanced features such as speaker diarization, automatic language identification, and word-level offsets.\n",
    "- `speechsdk.SpeechRecognizer`: Provides base speech transcription functionality.\n",
    "\n",
    "\n",
    "# Choosing Between ConversationTranscriber and SpeechRecognizer\n",
    "\n",
    "**SpeechRecognizer** is best for:\n",
    "- Simple, single-speaker transcription scenarios (e.g., dictation, command recognition).\n",
    "- Basic transcription tasks where advanced features are not required.\n",
    "- Use cases where you only need the recognized text and do not need speaker identification or advanced metadata.\n",
    "\n",
    "**ConversationTranscriber** is best for:\n",
    "- Multi-speaker conversations (e.g., meetings, interviews) where speaker diarization is needed.\n",
    "- Scenarios requiring word-level timestamps, automatic language identification, or more detailed recognition metadata.\n",
    "- Applications that need to distinguish between speakers and process advanced conversational features.\n",
    "\n",
    "\n",
    "# recognize_once_async\n",
    "\n",
    "The `SpeechRecognizer.recognize_once_async` method performs speech recognition in a non-blocking (asynchronous) mode, suitable for quick, single-shot transcription of short audio files or streams. It processes a single utterance, where the end of the utterance is automatically detected by either:\n",
    "\n",
    "- Listening for silence at the end of the speech.\n",
    "- Reaching a maximum duration of approximately 30 seconds of audio.\n",
    "\n",
    "This approach ensures fast transcription for short segments and is best suited for scenarios requiring capture of a single spoken phrase or command without waiting for longer audio input.\n",
    "\n",
    "Here’s how to use it:\n",
    "\n",
    "```python\n",
    "audio_input = speechsdk.AudioConfig(filename=\"output.wav\")\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "# result is of type speech_recognition_result\n",
    "result = speech_recognizer.recognize_once_async().get()\n",
    "```\n",
    "\n",
    "The recognized text can be retrieved from the `speech_recognition_result.text` property. For robust error handling and response management, it is essential to evaluate the `speech_recognition_result.reason` property and implement logic for each possible outcome:\n",
    "\n",
    "- **Recognized Speech** (`speechsdk.ResultReason.RecognizedSpeech`):\n",
    "  - Output the recognized text for further processing or display.\n",
    "- **No Match** (`speechsdk.ResultReason.NoMatch\n",
    "  - Notify the user or system that no speech could be recognized, enabling fallback or retry mechanisms.\n",
    "- **Recognition Canceled** (`speechsdk.ResultReason.Canceled`):\n",
    "  - Log the cancellation reason and error details for diagnostics.\n",
    "  - Advise on configuration issues, such as missing resource keys or endpoint values, to facilitate troubleshooting.\n",
    "\n",
    "Implementing structured error handling ensures application reliability, facilitates debugging, and provides meaningful feedback for both users and downstream systems.\n",
    "\n",
    "# Use continuous recognition vs recognize_once_async\n",
    "\n",
    "While `SpeechRecognizer.recognize_once_async` is ideal for quick, single-shot transcription of short audio files or streams. \n",
    "\n",
    "Continuous recongition provides \n",
    "- **Continuous Transcription**: Handles longer audio and ongoing conversations, not limited to a single utterance.\n",
    "- **Control stop recognizing** : continuous recognition is used when you want to control when to stop recognizing, possible using a seperate voice activity detector\n",
    "- **Real-Time Feedback**: Provides intermediate results and updates as the conversation progresses.\n",
    "- **Event-Driven Architecture**: Uses callback functions to handle events like transcribed text, cancellations, and session stops, allowing for more flexible and interactive workflows.\n",
    "\n",
    "The `speech.transcription.ConversationTranscriber` class will be used for realizing continuous recognition.\n",
    "To stop recognition, you must call stop_transcribing() or stop_transcribing_async(). \n",
    "\n",
    "## Note on Callback Functions\n",
    "Callback functions allow you to process transcribed text as it arrives, handle errors, and manage session lifecycle events. \n",
    "\n",
    "The following callback are available\n",
    "\n",
    "- **transcribing**: Triggered when intermediate transcription results are available. Useful for real-time feedback as speech is being processed.\n",
    "- **transcribed**: Triggered when a final transcription result is available. Use this to handle the completed transcription of an utterance.\n",
    "- **canceled**: Triggered when recognition is canceled due to errors or interruptions. Provides details for diagnostics and error handling.\n",
    "- **session_started**: Triggered when a recognition session starts. Useful for initializing resources or logging session activity.\n",
    "- **session_stopped**: Triggered when a recognition session stops. Use this to clean up resources or finalize session logs.\n",
    "- **speech_start_detected**: Triggered when the start of speech is detected in the audio stream. Can be used to mark the beginning of an utterance.\n",
    "- **speech_end_detected**: Triggered when the end of speech is detected. Useful for segmenting utterances and managing recognition boundaries.\n",
    "\n",
    "```python\n",
    "        conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "        ....\n",
    "\n",
    "        # Connect callbacks to ConversationTranscriber events\n",
    "        conversation_transcriber.transcribing.connect(_on_recognizing)\n",
    "        conversation_transcriber.transcribed.connect(_on_recognized)\n",
    "        conversation_transcriber.session_started.connect(_session_started)\n",
    "        conversation_transcriber.canceled.connect(canceled_callback)\n",
    "        conversation_transcriber.session_stopped.connect(session_stopped)\n",
    "\n",
    "        # Start continuous transcription\n",
    "        logger.info(\"starting \")\n",
    "        result_future=conversation_transcriber.start_transcribing_async()\n",
    "        \n",
    "        # Waits for completion.\n",
    "        while not transcribing_stop:\n",
    "            time.sleep(.5)      \n",
    "        \n",
    "        logger.info(\"completed transcribing\")\n",
    "        result=future=conversation_transcriber.stop_transcibing_aync()\n",
    "        result_future.get()\n",
    "        logger.info(\"recognition as started \")\n",
    "\n",
    "```\n",
    "\n",
    "# Alternate Hypothesis \n",
    "\n",
    "Alternate hypotheses in speech recognition refer to multiple possible transcriptions for a given audio segment, each with an associated confidence score. Instead of returning only the most likely transcript, Azure Speech to Text provides a ranked list of alternatives, allowing applications to access other plausible interpretations of the spoken input.\n",
    "\n",
    "Azure Speech to Text typically includes up to 5 alternate hypotheses for each utterance in the recognition result. These are found in the `NBest` array of the result's JSON property. Each hypothesis contains fields such as `Display`, `Lexical`, `Text`, `ITN`, and `Confidence`, enabling detailed analysis and selection based on application needs.\n",
    "\n",
    "The top hypothesis (highest confidence) is returned in `result.text`, while all alternatives can be accessed by parsing the `result.json` property.\n",
    "\n",
    "## Explanation of Fields in NBest\n",
    "\n",
    "Each element in the `NBest` array contains several fields:\n",
    "\n",
    "- **Display**: The formatted transcript as it would appear to a user, with punctuation and capitalization.\n",
    "- **Lexical**: The raw transcript with minimal formatting, typically all lowercase and without punctuation.\n",
    "- **Text**: Usually similar to `Display`, but may differ depending on the service version.\n",
    "- **ITN (Inverse Text Normalization)**: The transcript converted to a normalized form suitable for further processing (e.g., numbers as digits).\n",
    "- **Confidence**: A score (0.0 to 1.0) indicating the system's confidence in the accuracy of the hypothesis.\n",
    "\n",
    "To enable alternate hypothesis set the output format to `Detailed`:\n",
    "\n",
    "```python\n",
    "speech_config.output_format = speechsdk.OutputFormat.Detailed\n",
    "```\n",
    "\n",
    "The value returned in `result.text` is the transcript of the highest-confidence alternative, which corresponds to the `Display` field of the first element in the `NBest` array (`NBest[0]['Display']`). To access other alternatives, parse the `result.json` property.\n",
    "\n",
    "```python\n",
    "import json\n",
    "result_json = json.loads(result.json)\n",
    "# Print all alternate hypotheses in NBest\n",
    "if 'NBest' in json1 and json1['NBest']:\n",
    "    for idx, alt in enumerate(json1['NBest']):\n",
    "            display_text = alt.get('Display', alt.get('Lexical', alt.get('Text', '')))\n",
    "            confidence = alt.get('Confidence', None)\n",
    "            logger.info(f\"Alternative {idx+1}: {display_text} | Confidence: {confidence}\")\n",
    "                \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# Word-Level Timestamps\n",
    "\n",
    "Word-level timestamps provide precise timing information for each word in the transcribed text, including start and end times. This is useful for applications requiring detailed synchronization, such as:\n",
    "\n",
    "- Video captioning and subtitle alignment\n",
    "- Audio analysis and phoneme segmentation\n",
    "- Synchronizing transcripts with other media (e.g., video editing, search)\n",
    "- Building interactive transcripts and word-level navigation\n",
    "\n",
    "\n",
    "## Enable Word Level Transcripts \n",
    "\n",
    "To enable word-level timestamps in Azure Speech to Text: \n",
    "Set the `SpeechServiceResponse_RequestWordLevelTimestamps` property to \"true\" in the `SpeechConfig`.\n",
    "\n",
    "```python\n",
    "\n",
    "speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceResponse_RequestWordLevelTimestamps, \"true\"\n",
    "        )\n",
    "```\n",
    "\n",
    "## How Word-Level Offsets Work\n",
    "\n",
    "When you enable word-level timestamps in Azure Speech to Text, the recognition result includes a `json` property. This property contains a detailed JSON structure with timing data for each word. The relevant information is found in the `NBest` array:\n",
    "\n",
    "\n",
    "- **Words**: Each word object includes:\n",
    "    - `Word`: The recognized word\n",
    "    - `Offset`: The start time of the word (in 100-nanosecond ticks)\n",
    "    - `Duration`: The duration of the word (in ticks)\n",
    "    - `Confidence`: Confidence score for the word\n",
    "\n",
    "- **NBest**: This array contains alternative recognition hypotheses for the utterance. Each element includes a `Words` array, which holds word-level details. \n",
    "\n",
    "\n",
    "# Example: Extracting Word-Level Offsets\n",
    "\n",
    "```python\n",
    "import json\n",
    "result_json = json.loads(result.json)\n",
    "# Select only the highest-confidence alternative (NBest[0])\n",
    "if 'NBest' in result_json and result_json['NBest']:\n",
    "    best_alternative = result_json['NBest'][0]\n",
    "    for word in best_alternative.get('Words', []):\n",
    "        print(f\"Word: {word['Word']}, Start: {word['Offset']}, Duration: {word['Duration']}, Confidence: {word['Confidence']}\")\n",
    "```\n",
    "\n",
    "\n",
    "Word-level timestamps provide precise timing information for each word in the transcribed text, including start and end times. This is useful for applications requiring detailed synchronization, such as video captioning, audio analysis, or alignment with other media.\n",
    "\n",
    "\n",
    "# Speaker Diarization\n",
    "\n",
    "Diarization distinguishes between different speakers in a conversation. The speaker ID is a generic identifier assigned to each participant by the service during recognition as speakers are identified from the audio content.\n",
    "\n",
    "## Enabling Speaker Diarization\n",
    "\n",
    "```\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, \"true\"\n",
    "        )\n",
    "```\n",
    "The speaker ID for each recognized segment is available in the callback event as `evt.result.speaker_id`. You can access this property inside your transcribing or transcribed callback to identify which speaker spoke each part of the audio. This is useful for labeling transcript segments by speaker in multi-speaker scenarios.\n",
    "\n",
    "# Enable Language Detection\n",
    "\n",
    "Automatic language detection allows Azure Speech to Text to identify the spoken language in an audio stream without prior specification. This is useful for applications where the language may vary or is unknown at runtime.\n",
    "\n",
    "## How to Enable Automatic Language Detection\n",
    "\n",
    "To enable automatic language detection, use the `AutoDetectSourceLanguageConfig` class and pass it to the recognizer or transcriber. You can specify a list of possible languages to detect.\n",
    "\n",
    "```\n",
    "speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceConnection_LanguageIdMode, value='Continuous')\n",
    "\n",
    "auto_detect_source_language_config = speechsdk.languageconfig.AutoDetectSourceLanguageConfig(\n",
    "    languages=[\"en-US\", \"de-DE\", \"zh-CN\"])\n",
    "```\n",
    "\n",
    "## How to find the detected language from the result\n",
    "\n",
    "To parse the detected language from the results, you need to examine the JSON output for each turn of the conversation. In continuous recognition, the final results are typically an array of JSON objects, each representing a segment (turn) with its detected language and transcription details.\n",
    "\n",
    "Refer to the official documentation for more details: [Azure Speech Service Language Identification](https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-identification?pivots=programming-language-python&tabs=continuous)\n",
    "\n",
    "**Example: Parsing detected language from final results**\n",
    "\n",
    "```python\n",
    "# Assume final_results is a list of JSON objects, one per turn\n",
    "for turn in final_results:\n",
    "    # Each turn is a dict parsed from JSON\n",
    "    # The detected language is usually under 'PrimaryLanguage' or similar key\n",
    "    detected_lang = turn.get('PrimaryLanguage', {}).get('Language', None)\n",
    "    transcript = turn.get('DisplayText', '')\n",
    "    print(f\"Detected language: {detected_lang}, Transcript: {transcript}\")\n",
    "```\n",
    "\n",
    "Each JSON object may look like:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"PrimaryLanguage\": {\n",
    "    \"Language\": \"en-US\",\n",
    "    \"Confidence\": 0.98\n",
    "  },\n",
    "  \"DisplayText\": \"Hello, how are you?\",\n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "This allows you to extract the detected language and transcript for every segment in a multi-turn conversation.\n",
    "\n",
    "## Bilingual Detection\n",
    "\n",
    "Azure Speech to Text supports automatic detection among multiple languages, but it does not perform true bilingual (simultaneous multi-language) transcription within a single utterance. The service will select the most likely language from the provided list for each recognition session or utterance.\n",
    "\n",
    "If your use case involves code-switching (speakers switching between languages mid-sentence), the service will typically recognize only one language per utterance. For best results, provide a list of expected languages and segment audio where possible.\n",
    "\n",
    "- Specify up to 4 languages in the `AutoDetectSourceLanguageConfig`.\n",
    "- The detected language is returned in the recognition result properties.\n",
    "- For continuous recognition, language detection is performed per utterance.\n",
    "\n",
    "The link to complete source code can be found at \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Azure Speech to Text makes it easy to transcribe audio files and byte streams in Python, with powerful options for fast transcription and voice activity detection. Whether you’re building AI-powered apps, automating meeting notes, or processing audio data at scale, Azure’s SDK and APIs offer flexibility and accuracy.\n",
    "\n",
    "Sign up for my newsletter for the latest in AI, Python, and cloud engineering. Share your experiences or questions in the comments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 03:45:27,098 INFO: starting \n",
      "2025-10-06 03:45:27,098 INFO: Session started.\n",
      "2025-10-06 03:45:30,242 INFO: Recognizing: what's the weather like Speaker ID: Unknown\n",
      "2025-10-06 03:45:30,277 INFO: Recognizing: what's the weather like today Speaker ID: Unknown\n",
      "2025-10-06 03:45:30,279 INFO: Recognized: What's the weather like today? Speaker ID: Guest-1\n",
      "2025-10-06 03:45:30,706 INFO: Recognizing: 今天 Speaker ID: Unknown\n",
      "2025-10-06 03:45:30,899 INFO: Recognizing: 今天天气怎 Speaker ID: Unknown\n",
      "2025-10-06 03:45:30,920 INFO: Recognized: 今天天气怎么样？ Speaker ID: Guest-1\n",
      "2025-10-06 03:45:31,162 INFO: Recognizing: how do i go to that Speaker ID: Unknown\n",
      "2025-10-06 03:45:31,267 INFO: Recognizing: how do i go to that bus stop Speaker ID: Unknown\n",
      "2025-10-06 03:45:31,310 INFO: Recognized: How do I go to that bus stop? Speaker ID: Guest-1\n",
      "2025-10-06 03:45:31,437 INFO: Recognizing: 请问 Speaker ID: Unknown\n",
      "2025-10-06 03:45:31,636 INFO: Recognizing: 请问那个车站 Speaker ID: Unknown\n",
      "2025-10-06 03:45:31,730 INFO: Recognizing: 请问那个车站怎么走 Speaker ID: Unknown\n",
      "2025-10-06 03:45:31,888 INFO: Recognized: 请问那个车站怎么走？ Speaker ID: Guest-1\n",
      "2025-10-06 03:45:32,139 INFO: Recognizing: can you Speaker ID: Unknown\n",
      "2025-10-06 03:45:32,647 INFO: Recognizing: can you tell me Speaker ID: Unknown\n",
      "2025-10-06 03:45:32,749 INFO: Recognizing: can you tell me where is the Speaker ID: Unknown\n",
      "2025-10-06 03:45:32,850 INFO: Recognizing: can you tell me where is the library Speaker ID: Unknown\n",
      "2025-10-06 03:45:33,171 INFO: Recognized: Can you tell me where is the library? Speaker ID: Guest-1\n",
      "2025-10-06 03:45:34,007 INFO: Recognizing: she went to Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,207 INFO: Recognizing: she went to shu Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,300 INFO: Recognizing: she went to shukran Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,442 INFO: Recognizing: she went to Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,444 INFO: Recognized: She went to. Speaker ID: Guest-1\n",
      "2025-10-06 03:45:34,584 INFO: Recognizing: 图书馆 Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,682 INFO: Recognizing: 图书馆在哪里 Speaker ID: Unknown\n",
      "2025-10-06 03:45:34,854 INFO: Recognized: 图书馆在哪里？ Speaker ID: Guest-1\n",
      "2025-10-06 03:45:35,549 INFO: Recognized:  Speaker ID: Unknown\n",
      "2025-10-06 03:45:36,154 INFO: Canceled: ConversationTranscriptionCanceledEventArgs(session_id=5c846da5f01e4d979c75fe580d8f6c73, result=ConversationTranscriptionResult(result_id=21a8d0768de0410c8f06c9ca7ed5a245, speaker_id=, text=, reason=ResultReason.Canceled))\n",
      "2025-10-06 03:45:36,156 INFO: Session stopped.\n",
      "2025-10-06 03:45:36,170 INFO: completed transcribing\n",
      "2025-10-06 03:45:36,171 INFO: stopping transcribing\n",
      "2025-10-06 03:45:36,172 INFO: [{'Id': '52f0316a49fa4e4fad7c31fdd1a15546', 'RecognitionStatus': 'Success', 'DisplayText': \"What's the weather like today?\", 'Offset': 700000, 'Duration': 12000000, 'PrimaryLanguage': {'Language': 'en-US', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': '290f701c392345818f566423b03bb10d', 'RecognitionStatus': 'Success', 'DisplayText': '今天天气怎么样？', 'Offset': 29480000, 'Duration': 11600000, 'PrimaryLanguage': {'Language': 'zh-CN', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': '0cd97d5f65f14878bff9d32361392041', 'RecognitionStatus': 'Success', 'DisplayText': 'How do I go to that bus stop?', 'Offset': 62680000, 'Duration': 16400000, 'PrimaryLanguage': {'Language': 'en-US', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': 'ac81b570d07047919cb38f0d3f1e56fc', 'RecognitionStatus': 'Success', 'DisplayText': '请问那个车站怎么走？', 'Offset': 97880000, 'Duration': 16000000, 'PrimaryLanguage': {'Language': 'zh-CN', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': '084b75c5b9fb42d295d7adb1bf01bc11', 'RecognitionStatus': 'Success', 'DisplayText': 'Can you tell me where is the library?', 'Offset': 135880000, 'Duration': 23200000, 'PrimaryLanguage': {'Language': 'en-US', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': '40cd01ef1fd64874b9ecb087f24ffe99', 'RecognitionStatus': 'Success', 'DisplayText': 'She went to.', 'Offset': 170280000, 'Duration': 6400000, 'PrimaryLanguage': {'Language': 'en-US', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': 'c03b132ae96542cba4744951d2ca5921', 'RecognitionStatus': 'Success', 'DisplayText': '图书馆在哪里？', 'Offset': 175480000, 'Duration': 9600000, 'PrimaryLanguage': {'Language': 'zh-CN', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Guest-1'}, {'Id': 'cb13055feb11494989ce1244cf5d3dea', 'RecognitionStatus': 'Success', 'DisplayText': '', 'Offset': 185080000, 'Duration': 15200000, 'PrimaryLanguage': {'Language': 'en-US', 'Confidence': 'Unknown'}, 'Channel': 0, 'Type': 'ConversationTranscription', 'SpeakerId': 'Unknown'}]\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py\", line 1150, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py\", line 998, in format\n",
      "    return fmt.format(record)\n",
      "           ~~~~~~~~~~^^^^^^^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py\", line 711, in format\n",
      "    record.message = record.getMessage()\n",
      "                     ~~~~~~~~~~~~~~~~~^^\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/logging/__init__.py\", line 400, in getMessage\n",
      "    msg = msg % self.args\n",
      "          ~~~~^~~~~~~~~~~\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\", line 683, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/base_events.py\", line 2040, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/asyncio/events.py\", line 89, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/ipykernel/zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/pi/Library/Mobile Documents/com~apple~CloudDocs/workspace/LL/cw/neural-engineer/codex/update-collect_all_indicators-method-arguments/venv-py310/lib/python3.13/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/wy/xyg009356vlb9p2k691ggv780000gp/T/ipykernel_50139/2482154041.py\", line 217, in <module>\n",
      "    final_results=recognize_speech(source=\"sample3.wav\",mode=\"continuous\",word_timestamps=False,language_id_mode=[\"en-US\",\"zh-CN\"],alternative_hypotheses=False)\n",
      "  File \"/var/folders/wy/xyg009356vlb9p2k691ggv780000gp/T/ipykernel_50139/2482154041.py\", line 211, in recognize_speech\n",
      "    logger.info(\"output text\",s1)\n",
      "Message: 'output text'\n",
      "Arguments: ([\"What's the weather like today?\", '今天天气怎么样？', 'How do I go to that bus stop?', '请问那个车站怎么走？', 'Can you tell me where is the library?', 'She went to.', '图书馆在哪里？', ''],)\n",
      "2025-10-06 03:45:36,187 INFO: completed script\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 03:46:16,734 INFO: Recognizing: what's the weather like Speaker ID: Unknown\n",
      "2025-10-06 03:46:16,767 INFO: Recognizing: what's the weather like today Speaker ID: Unknown\n",
      "2025-10-06 03:46:16,768 INFO: Recognized: What's the weather like today? Speaker ID: Guest-1\n",
      "2025-10-06 03:47:07,903 INFO: Recognizing: jing jing Speaker ID: Unknown\n",
      "2025-10-06 03:47:23,706 INFO: Recognizing: 今天 Speaker ID: Unknown\n",
      "2025-10-06 03:47:27,103 INFO: Recognizing: 今天天气怎 Speaker ID: Unknown\n",
      "2025-10-06 03:47:33,492 INFO: Recognized: 今天天气怎么样？ Speaker ID: Guest-1\n",
      "2025-10-06 03:48:39,617 INFO: Recognizing: how Speaker ID: Unknown\n",
      "2025-10-06 03:48:43,414 INFO: Recognizing: how do Speaker ID: Unknown\n",
      "2025-10-06 03:48:48,014 INFO: Recognizing: how do i Speaker ID: Unknown\n",
      "2025-10-06 03:48:50,414 INFO: Recognizing: how do i go Speaker ID: Unknown\n",
      "2025-10-06 03:48:50,522 INFO: Recognizing: how do i go to Speaker ID: Unknown\n",
      "2025-10-06 03:48:56,124 INFO: Recognizing: how do i go to that Speaker ID: Unknown\n",
      "2025-10-06 03:49:03,620 INFO: Recognizing: how do i go to that bus stop Speaker ID: Unknown\n",
      "2025-10-06 03:49:13,252 INFO: Recognized: How do I go to that bus stop? Speaker ID: Guest-1\n",
      "2025-10-06 03:50:44,359 INFO: Recognizing: she went down Speaker ID: Unknown\n",
      "2025-10-06 03:50:49,262 INFO: Recognizing: 请问 Speaker ID: Unknown\n",
      "2025-10-06 03:50:49,453 INFO: Recognizing: 请问那个 Speaker ID: Unknown\n",
      "2025-10-06 03:50:59,860 INFO: Recognizing: 请问那个车站怎么走 Speaker ID: Unknown\n",
      "2025-10-06 03:51:24,101 INFO: Recognized: 请问那个车站怎么走？ Speaker ID: Guest-1\n",
      "2025-10-06 03:52:11,461 INFO: Recognizing: can Speaker ID: Unknown\n",
      "2025-10-06 03:52:14,463 INFO: Recognizing: can you Speaker ID: Unknown\n",
      "2025-10-06 03:52:50,977 INFO: Recognizing: can you tell me Speaker ID: Unknown\n",
      "2025-10-06 03:52:51,063 INFO: Recognizing: can you tell me where is the Speaker ID: Unknown\n",
      "2025-10-06 03:53:07,461 INFO: Recognizing: can you tell me where is the library Speaker ID: Unknown\n",
      "2025-10-06 03:53:25,511 INFO: Recognized: Can you tell me where is the library? Speaker ID: Guest-1\n",
      "2025-10-06 03:54:12,990 INFO: Recognizing: she went to Speaker ID: Unknown\n",
      "2025-10-06 03:54:25,505 INFO: Recognizing: she went to shukran Speaker ID: Unknown\n",
      "2025-10-06 03:54:37,401 INFO: Recognizing: she went to shukran said nani Speaker ID: Unknown\n",
      "2025-10-06 03:54:40,236 INFO: Recognized: She went to. Speaker ID: Guest-1\n",
      "2025-10-06 03:54:40,352 INFO: Recognizing: 图书馆 Speaker ID: Unknown\n",
      "2025-10-06 03:54:40,444 INFO: Recognizing: 图书馆在哪里 Speaker ID: Unknown\n",
      "2025-10-06 03:54:42,934 INFO: Recognized: 图书馆在哪里？ Speaker ID: Guest-1\n",
      "2025-10-06 03:55:38,575 INFO: Recognized:  Speaker ID: Unknown\n",
      "2025-10-06 03:55:41,094 INFO: Canceled: ConversationTranscriptionCanceledEventArgs(session_id=e84f217f86004c66b650656a421adb5b, result=ConversationTranscriptionResult(result_id=f9d0fa82f6c8428b83829557a4cf2e20, speaker_id=, text=, reason=ResultReason.Canceled))\n",
      "2025-10-06 03:55:41,096 INFO: Session stopped.\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import threading\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# Setup logger with timestamp\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "speech_key = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "service_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "endpoint_id = os.getenv(\"AZURE_SPEECH_ENDPOINT_ID\")\n",
    "\n",
    "\n",
    "\n",
    "def recognize_speech(mode=\"recognize_speech_once\",source=\"sample1.wav\",alternative_hypotheses=False,word_timestamps=False,diarize_intermediate=False,language_id_mode=None):\n",
    "    # File input for ConversationTranscriber\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "    #speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, value='true')\n",
    "    speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "\n",
    "\n",
    "    audio_input = speechsdk.AudioConfig(filename=source)\n",
    "    \n",
    "    if word_timestamps == True:\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceResponse_RequestWordLevelTimestamps, \"true\"\n",
    "        )\n",
    "    if diarize_intermediate == True:\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceResponse_DiarizeIntermediateResults, \"true\"\n",
    "        )\n",
    "    if language_id_mode is not None:\n",
    "        try:\n",
    "            speech_config.set_property(property_id=speechsdk.PropertyId.SpeechServiceConnection_LanguageIdMode, value='Continuous')\n",
    "            speech_config.auto_detect_source_language_config = speechsdk.languageconfig.AutoDetectSourceLanguageConfig(languages=language_id_mode)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if alternative_hypotheses == True:\n",
    "        speech_config.output_format = speechsdk.OutputFormat.Detailed \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if mode == \"recognize_speech_once\":\n",
    "        \n",
    "        speech_config.output_format = speechsdk.OutputFormat.Detailed\n",
    "        \n",
    "        speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)\n",
    "\n",
    "        # Add timeout logic to recognize_once_async\n",
    "        try:\n",
    "            result = speech_recognizer.recognize_once_async().get()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Recognition failed or timed out: {e}\")\n",
    "            print(\"Recognition failed or timed out.\")\n",
    "            return\n",
    "\n",
    "        raw_json = getattr(result, \"json\", \"\")\n",
    "        logger.info(f\"Raw JSON: {raw_json}\")\n",
    "        \n",
    "        if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "            print(\"Recognized: {}\".format(result.text))\n",
    "        elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "            print(\"No speech could be recognized.\")\n",
    "        elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "            cancellation_details = result.cancellation_details\n",
    "            print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "            if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                print(\"Did you set the speech resource key and endpoint values?\")\n",
    "                \n",
    "        json1=json.loads(raw_json)\n",
    "        # Print all alternate hypotheses in NBest\n",
    "        if 'NBest' in json1 and json1['NBest']:\n",
    "            for idx, alt in enumerate(json1['NBest']):\n",
    "                display_text = alt.get('Display', alt.get('Lexical', alt.get('Text', '')))\n",
    "                confidence = alt.get('Confidence', None)\n",
    "                print(f\"Alternative {idx+1}: {display_text} | Confidence: {confidence}\")\n",
    "                \n",
    "                \n",
    "    else :\n",
    "\n",
    "        conversation_transcriber = speechsdk.transcription.ConversationTranscriber(speech_config=speech_config, audio_config=audio_input,\n",
    "        auto_detect_source_language_config=speech_config.auto_detect_source_language_config if language_id_mode is not None else None\n",
    "         )\n",
    "\n",
    "\n",
    "        results = []\n",
    "        final_results = []\n",
    "        transcribing_stop = False\n",
    "\n",
    "        def _on_recognizing(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "            try:\n",
    "                res=evt.result\n",
    "                logger.info(f\"Recognizing: {evt.result.text} Speaker ID: {evt.result.speaker_id}\")\n",
    "                \n",
    "                raw_json = getattr(res, \"json\", \"\")\n",
    "                if raw_json:\n",
    "                    payload = json.loads(raw_json)\n",
    "                else:    \n",
    "                    payload = {\"text\": res.text, \"start_ts\": float(getattr(res, \"offset\", 0)) / 10_000_000.0 , \"speaker_id\": res.speaker_id}\n",
    "                results.append(payload)\n",
    "                \n",
    "                \n",
    "                if res.reason is not None:\n",
    "                    if res.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "                        print(\"Recognized: {}\".format(res.text))\n",
    "                    elif res.reason == speechsdk.ResultReason.NoMatch:\n",
    "                        print(\"No speech could be recognized.\")\n",
    "                    elif res.reason == speechsdk.ResultReason.Canceled:\n",
    "                        cancellation_details = res.cancellation_details\n",
    "                        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "                        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "                            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "                            print(\"Did you set the speech resource key and endpoint values?\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            #print('\\tSpeaker ID={}'.format(evt.result.speaker_id))\n",
    "\n",
    "        # Callback for final results\n",
    "        def _on_recognized(evt: speechsdk.SpeechRecognitionEventArgs):\n",
    "            #logger.info(f\"Recognized: {evt.result.text}\")\n",
    "            \n",
    "            #auto_detect_source_language_result = speechsdk.AutoDetectSourceLanguageResult(evt.result)\n",
    "            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "                res=evt.result\n",
    "                raw_json = getattr(res, \"json\", \"\")\n",
    "                if raw_json:\n",
    "                    payload = json.loads(raw_json)\n",
    "                else:    \n",
    "                    payload = {\"text\": res.text, \"start_ts\": float(getattr(res, \"offset\", 0)) / 10_000_000.0 , \"speaker_id\": res.speaker_id}\n",
    "                # Check if payload is a list (JSON array)\n",
    "                if isinstance(payload, list):\n",
    "                    final_results.extend(payload)\n",
    "                else:\n",
    "                    final_results.append(payload)\n",
    "                logger.info(f\"Recognized: {evt.result.text} Speaker ID: {evt.result.speaker_id}\")\n",
    "            elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "                print('\\tNOMATCH: Speech could not be TRANSCRIBED: {}'.format(evt.result.no_match_details))\n",
    "\n",
    "        # Callback for session started\n",
    "        def _session_started(evt: speechsdk.SessionEventArgs):\n",
    "            logger.info(\"Session started.\")\n",
    "\n",
    "        # Callback for session stopped\n",
    "        def session_stopped(evt: speechsdk.SessionEventArgs):\n",
    "            logger.info(\"Session stopped.\")\n",
    "            nonlocal transcribing_stop\n",
    "            transcribing_stop=True\n",
    "\n",
    "        # Callback for cancellation\n",
    "        def canceled_callback(evt: speechsdk.SessionEventArgs):\n",
    "            logger.info(f\"Canceled: {evt}\")\n",
    "            nonlocal transcribing_stop\n",
    "            transcribing_stop=True\n",
    "\n",
    "        # Connect callbacks to ConversationTranscriber events\n",
    "        conversation_transcriber.transcribing.connect(_on_recognizing)\n",
    "\n",
    "        conversation_transcriber.transcribed.connect(_on_recognized)\n",
    "\n",
    "        conversation_transcriber.session_started.connect(_session_started)\n",
    "        conversation_transcriber.canceled.connect(canceled_callback)\n",
    "        conversation_transcriber.session_stopped.connect(session_stopped)\n",
    "\n",
    "        # Start continuous transcription\n",
    "        logger.info(\"starting \")\n",
    "        result_future=conversation_transcriber.start_transcribing_async()\n",
    "        \n",
    "        # Waits for completion.\n",
    "        while not transcribing_stop:\n",
    "            time.sleep(.5)      \n",
    "   \n",
    "        logger.info(\"completed transcribing\")\n",
    "        result_future=conversation_transcriber.stop_transcribing_async()\n",
    "        logger.info(\"stopping transcribing\")\n",
    "        result_future.get()\n",
    "             \n",
    "        logger.info(final_results)\n",
    "        \n",
    "        s1=[]\n",
    "        for s in final_results:\n",
    "          s1.append(s[\"DisplayText\"])  \n",
    "        \n",
    "        json1=final_results[0]\n",
    "        if 'NBest' in json1 and json1['NBest']:\n",
    "            for idx, alt in enumerate(json1['NBest']):\n",
    "                display_text = alt.get('Display', alt.get('Lexical', alt.get('Text', '')))\n",
    "                confidence = alt.get('Confidence', None)\n",
    "                logger.info(f\"Alternative {idx+1}: {display_text} | Confidence: {confidence}\")\n",
    "  \n",
    "            best_alternative = json1['NBest'][0]\n",
    "            for word in best_alternative.get('Words', []):\n",
    "                print(f\"Word: {word['Word']}, Start: {word['Offset']}, Duration: {word['Duration']}, Confidence: {word['Confidence']}\")              \n",
    "        \n",
    "        logger.info(\"output text\",s1)\n",
    "        logger.info(\"completed script\")\n",
    "        return final_results\n",
    "\n",
    "try:\n",
    "     \n",
    "     final_results=recognize_speech(source=\"sample3.wav\",mode=\"continuous\",word_timestamps=False,language_id_mode=[\"en-US\",\"zh-CN\"],alternative_hypotheses=False)\n",
    "except Exception as e :\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
