{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f03606",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install azure-cognitiveservices-speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46c0450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "import os\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import logging\n",
    "\n",
    "transcribing_stop=False\n",
    "# Setup logger with timestamp\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s: %(message)s', level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AzureConversationTranscriber:\n",
    "    \"\"\"\n",
    "    Azure Conversation Transcriber with configurable VAD parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        subscription_key: str,\n",
    "        region: str,\n",
    "        language: str = \"en-US\"\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the conversation transcriber\n",
    "        \n",
    "        Args:\n",
    "            subscription_key: Azure Speech Services subscription key\n",
    "            region: Azure region (e.g., 'eastus', 'westus')\n",
    "            language: Speech recognition language code\n",
    "        \"\"\"\n",
    "        self.subscription_key = subscription_key\n",
    "        self.region = region\n",
    "        self.language = language\n",
    "        self.transcriber: Optional[speechsdk.transcription.ConversationTranscriber] = None\n",
    "        \n",
    "    def configure_speech_config(\n",
    "        self,\n",
    "        initial_silence_timeout_ms: int = 5000,\n",
    "        segmentation_silence_timeout_ms: int = 1500,\n",
    "        segmentation_max_time_ms: Optional[int] = None,\n",
    "        start_sensitivity: str = \"medium\",\n",
    "        segmentation_strategy: str = \"Time\"\n",
    "    ) -> speechsdk.SpeechConfig:\n",
    "        \"\"\"\n",
    "        Configure speech recognition with comprehensive VAD parameters\n",
    "        \n",
    "        Args:\n",
    "            initial_silence_timeout_ms: Time to wait for speech to begin (default: 5000ms)\n",
    "                - Range: 2000-15000ms\n",
    "                - Use lower for fast interactions, higher for thoughtful responses\n",
    "            \n",
    "            segmentation_silence_timeout_ms: Silence duration to end utterance (default: 1500ms)\n",
    "                - Range: 500-5000ms\n",
    "                - Lower = faster response but may cut speech, Higher = complete utterances\n",
    "            \n",
    "            segmentation_max_time_ms: Maximum single utterance duration (optional)\n",
    "                - Range: 5000-30000ms\n",
    "                - Forces segmentation after this time regardless of pauses\n",
    "            \n",
    "            start_sensitivity: Speech start detection sensitivity (default: \"medium\")\n",
    "                - Options: \"low\", \"medium\", \"high\"\n",
    "                - high = faster detection, low = fewer false positives\n",
    "            \n",
    "            segmentation_strategy: How to segment speech (default: \"Time\")\n",
    "                - Options: \"Automatic\", \"Time\", \"Semantic\"\n",
    "                - \"Time\" recommended for predictable behavior\n",
    "            \n",
    "        Returns:\n",
    "            Configured SpeechConfig object\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(\"Configuring SpeechConfig with VAD parameters\")\n",
    "        logger.debug(f\"Initial Silence Timeout: {self.subscription_key} ms\")\n",
    "        \n",
    "        speech_config = speechsdk.SpeechConfig(\n",
    "            subscription=self.subscription_key,\n",
    "            region=self.region\n",
    "        )\n",
    "        \n",
    "        # Set the recognition language\n",
    "        speech_config.speech_recognition_language = self.language\n",
    "        \n",
    "        # 1. Configure initial silence timeout - how long to wait for speech to begin\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.SpeechServiceConnection_InitialSilenceTimeoutMs,\n",
    "            str(initial_silence_timeout_ms)\n",
    "        )\n",
    "        \n",
    "        # 2. Configure segmentation strategy - use time-based for predictable behavior\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.Speech_SegmentationStrategy,\n",
    "            segmentation_strategy\n",
    "        )\n",
    "        \n",
    "        # 3. Configure speech start detection sensitivity\n",
    "        # Validates and sets the sensitivity level\n",
    "        valid_sensitivities = [\"low\", \"medium\", \"high\"]\n",
    "        if start_sensitivity not in valid_sensitivities:\n",
    "            raise ValueError(f\"start_sensitivity must be one of {valid_sensitivities}\")\n",
    "        \n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.Speech_StartEventSensitivity,\n",
    "            start_sensitivity\n",
    "        )\n",
    "        \n",
    "        # 4. Configure segmentation silence timeout - silence duration to end utterance\n",
    "        speech_config.set_property(\n",
    "            speechsdk.PropertyId.Speech_SegmentationSilenceTimeoutMs,\n",
    "            str(segmentation_silence_timeout_ms)\n",
    "        )\n",
    "        \n",
    "        # 5. Optional: Configure maximum utterance duration\n",
    "        if segmentation_max_time_ms:\n",
    "            speech_config.set_property(\n",
    "                speechsdk.PropertyId.Speech_SegmentationMaximumTimeMs,\n",
    "                str(segmentation_max_time_ms)\n",
    "            )\n",
    "        \n",
    "        # Enable detailed result output for better analysis\n",
    "        speech_config.output_format = speechsdk.OutputFormat.Detailed\n",
    "        \n",
    "        # # Enable speaker diarization for conversation scenarios\n",
    "        # speech_config.set_property(\n",
    "        #     speechsdk.PropertyId.SpeechServiceConnection_RecoMode,\n",
    "        #     \"CONVERSATION\"\n",
    "        # )\n",
    "        \n",
    "        return speech_config\n",
    "        \n",
    "    def create_conversation_transcriber(\n",
    "        self,\n",
    "        audio_config: Optional[speechsdk.AudioConfig] = None,\n",
    "        initial_silence_timeout_ms: int = 5000,\n",
    "        segmentation_silence_timeout_ms: int = 1500,\n",
    "        segmentation_max_time_ms: Optional[int] = None,\n",
    "        start_sensitivity: str = \"medium\",\n",
    "        segmentation_strategy: str = \"Time\"\n",
    "    ) -> speechsdk.transcription.ConversationTranscriber:\n",
    "        \"\"\"\n",
    "        Create and configure a conversation transcriber with full VAD control\n",
    "        \n",
    "        Args:\n",
    "            audio_config: Audio input configuration (default: microphone)\n",
    "            initial_silence_timeout_ms: Wait time before speech starts (default: 5000ms)\n",
    "            segmentation_silence_timeout_ms: Silence to end utterance (default: 1500ms)\n",
    "            segmentation_max_time_ms: Max utterance duration (optional)\n",
    "            start_sensitivity: Detection sensitivity - \"low\"/\"medium\"/\"high\"\n",
    "            segmentation_strategy: Segmentation method - \"Automatic\"/\"Time\"/\"Semantic\"\n",
    "            \n",
    "        Returns:\n",
    "            Configured ConversationTranscriber instance\n",
    "        \"\"\"\n",
    "        # Configure speech settings with all VAD parameters\n",
    "        speech_config = self.configure_speech_config(\n",
    "            initial_silence_timeout_ms=initial_silence_timeout_ms,\n",
    "            segmentation_silence_timeout_ms=segmentation_silence_timeout_ms,\n",
    "            segmentation_max_time_ms=segmentation_max_time_ms,\n",
    "            start_sensitivity=start_sensitivity,\n",
    "            segmentation_strategy=segmentation_strategy\n",
    "        )\n",
    "        \n",
    "        # Use default microphone if no audio config provided\n",
    "        if audio_config is None:\n",
    "            audio_config = speechsdk.AudioConfig(use_default_microphone=True)\n",
    "        \n",
    "        # Create conversation transcriber\n",
    "        self.transcriber = speechsdk.transcription.ConversationTranscriber(\n",
    "            speech_config=speech_config,\n",
    "            audio_config=audio_config\n",
    "        )\n",
    "        \n",
    "        return self.transcriber\n",
    "        \n",
    "    def setup_event_handlers(\n",
    "        self,\n",
    "        on_transcribed: Optional[callable] = None,\n",
    "        on_transcribing: Optional[callable] = None,\n",
    "        on_session_started: Optional[callable] = None,\n",
    "        on_session_stopped: Optional[callable] = None,\n",
    "        on_canceled: Optional[callable] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Setup event handlers for the transcriber\n",
    "        \n",
    "        Args:\n",
    "            on_transcribed: Handler for final transcription results\n",
    "            on_transcribing: Handler for interim transcription results\n",
    "            on_session_started: Handler for session start event\n",
    "            on_session_stopped: Handler for session stop event\n",
    "            on_canceled: Handler for cancellation events\n",
    "        \"\"\"\n",
    "        if not self.transcriber:\n",
    "            raise ValueError(\"Transcriber not initialized. Call create_conversation_transcriber first.\")\n",
    "        \n",
    "        # Default handlers if none provided\n",
    "        def default_transcribed(evt):\n",
    "            if evt.result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "                logger.info(f\"[TRANSCRIBED] Speaker {evt.result.speaker_id}: {evt.result.text}\")\n",
    "            elif evt.result.reason == speechsdk.ResultReason.NoMatch:\n",
    "                logger.warning(f\"[NO MATCH] Speech could not be recognized\")\n",
    "        \n",
    "        def default_transcribing(evt):\n",
    "            logger.info(f\"[TRANSCRIBING] {evt.result.text}\")\n",
    "        \n",
    "        def default_session_started(evt):\n",
    "            logger.info(f\"[SESSION STARTED] Session ID: {evt.session_id}\")\n",
    "        \n",
    "        def default_session_stopped(evt):\n",
    "            logger.info(f\"[SESSION STOPPED] Session ID: {evt.session_id}\")\n",
    "            transcribing_stop=True\n",
    "        \n",
    "        def default_canceled(evt):\n",
    "            logger.warning(f\"[CANCELED] Reason: {evt.result.reason}\")\n",
    "            if evt.result.reason == speechsdk.ResultReason.Canceled:\n",
    "                cancellation = evt.result.cancellation_details\n",
    "                logger.error(f\"  Error details: {cancellation.error_details}\")\n",
    "            transcribing_stop=True\n",
    "        \n",
    "        # Connect event handlers\n",
    "        self.transcriber.transcribed.connect(\n",
    "            on_transcribed if on_transcribed else default_transcribed\n",
    "        )\n",
    "        self.transcriber.transcribing.connect(\n",
    "            on_transcribing if on_transcribing else default_transcribing\n",
    "        )\n",
    "        self.transcriber.session_started.connect(\n",
    "            on_session_started if on_session_started else default_session_started\n",
    "        )\n",
    "        self.transcriber.session_stopped.connect(\n",
    "            on_session_stopped if on_session_stopped else default_session_stopped\n",
    "        )\n",
    "        self.transcriber.canceled.connect(\n",
    "            on_canceled if on_canceled else default_canceled\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4219a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_transcriber():\n",
    "    \"\"\"\n",
    "    Initialize and run Azure Conversation Transcriber for audio file transcription.\n",
    "    \n",
    "    This function is optimized for fast-paced customer service interactions with:\n",
    "    - Quick speech detection (5 second initial timeout)\n",
    "    - Fast response segmentation (800ms silence to end utterance)\n",
    "    - Time-based segmentation strategy for predictable behavior\n",
    "    \n",
    "    The function performs the following steps:\n",
    "    1. Loads Azure credentials from environment variables\n",
    "    2. Creates an Azure transcriber instance with US English language\n",
    "    3. Configures VAD (Voice Activity Detection) parameters\n",
    "    4. Sets up event handlers for transcription events\n",
    "    5. Starts asynchronous transcription\n",
    "    6. Waits for user input to stop transcription\n",
    "    \n",
    "    Environment Variables Required:\n",
    "        AZURE_SPEECH_KEY: Azure Speech Services subscription key\n",
    "        AZURE_SPEECH_REGION: Azure region (e.g., 'eastus', 'westus')\n",
    "        AZURE_SPEECH_ENDPOINT_ID: (Optional) Custom endpoint ID\n",
    "    \n",
    "    Global Variables:\n",
    "        transcribing_stop: Boolean flag to control transcription loop\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: If environment variables are not set\n",
    "        Exception: If transcription fails to start\n",
    "    \"\"\"\n",
    "    \n",
    "    # Import required modules for environment variable management\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    # Load environment variables from .env file\n",
    "    # This allows secure storage of API keys outside the code\n",
    "    load_dotenv()\n",
    "    \n",
    "    # Retrieve Azure Speech Services credentials from environment\n",
    "    # These are required for authentication with Azure services\n",
    "    speech_key = os.getenv(\"AZURE_SPEECH_KEY\")\n",
    "    service_region = os.getenv(\"AZURE_SPEECH_REGION\")\n",
    "    endpoint_id = os.getenv(\"AZURE_SPEECH_ENDPOINT_ID\")  # Optional: for custom models\n",
    "\n",
    "    # Initialize the Azure Conversation Transcriber\n",
    "    # This wrapper class manages speech recognition with configurable VAD parameters\n",
    "    transcriber = AzureConversationTranscriber(\n",
    "        subscription_key=os.getenv(\"AZURE_SPEECH_KEY\"),  # Azure subscription key\n",
    "        region=os.getenv(\"AZURE_SPEECH_REGION\"),          # Azure deployment region\n",
    "        language=\"en-US\"                                   # Speech recognition language\n",
    "    )\n",
    "    \n",
    "    # Configure audio input from a WAV file\n",
    "    # Alternative: use default microphone with AudioConfig(use_default_microphone=True)\n",
    "    audio_input = speechsdk.AudioConfig(filename=\"sample1.wav\")\n",
    "    \n",
    "    # Create and configure the conversation transcriber with VAD parameters\n",
    "    transcriber.create_conversation_transcriber(\n",
    "        audio_config=audio_input,\n",
    "        \n",
    "        # Initial silence timeout: How long to wait for speech to begin (in milliseconds)\n",
    "        # 5000ms = 5 seconds - Reasonable wait time before assuming no speech\n",
    "        initial_silence_timeout_ms=5000,\n",
    "        \n",
    "        # Segmentation silence timeout: Duration of silence to end an utterance\n",
    "        # 800ms = 0.8 seconds - Quick response time for fast-paced conversations\n",
    "        # Lower values = faster response but may cut off speech\n",
    "        # Higher values = more complete utterances but slower response\n",
    "        segmentation_silence_timeout_ms=800,\n",
    "        \n",
    "        # Optional parameters (currently commented out):\n",
    "        # segmentation_max_time_ms: Maximum duration for a single utterance (20s)\n",
    "        #   Forces segmentation after this time regardless of pauses\n",
    "        # start_sensitivity: Detection sensitivity (\"low\"/\"medium\"/\"high\")\n",
    "        #   \"high\" = faster detection but more false positives\n",
    "        \n",
    "        # Segmentation strategy: How to divide continuous speech into segments\n",
    "        # \"Time\" = Use time-based rules for predictable, consistent behavior\n",
    "        # Alternative: \"Automatic\" (Azure decides) or \"Semantic\" (natural language breaks)\n",
    "        segmentation_strategy=\"Time\"\n",
    "    )\n",
    "    \n",
    "    # Display configuration confirmation to user\n",
    "    print(\"Customer Service Mode: Fast response optimized\")\n",
    "    \n",
    "    # Setup default event handlers for transcription events\n",
    "    # These handlers log:\n",
    "    # - transcribing: Interim results as speech is being processed\n",
    "    # - transcribed: Final transcription results with speaker identification\n",
    "    # - session_started/stopped: Session lifecycle events\n",
    "    # - canceled: Error conditions and cancellation details\n",
    "    transcriber.setup_event_handlers()\n",
    "    \n",
    "    # Start asynchronous transcription\n",
    "    # The .get() method blocks until the async operation completes\n",
    "    # This begins processing the audio file and triggering events\n",
    "    transcriber.transcriber.start_transcribing_async().get()\n",
    "    \n",
    "    # Main transcription loop\n",
    "    # Continuously checks the global flag 'transcribing_stop'\n",
    "    # The flag is set to True by event handlers when:\n",
    "    # - Session stops naturally (end of audio file)\n",
    "    # - An error/cancellation occurs\n",
    "    while not transcribing_stop:\n",
    "        time.sleep(.5)  # Sleep 500ms to avoid busy-waiting and reduce CPU usage\n",
    "    \n",
    "    # Wait for user confirmation before stopping\n",
    "    # Allows user to review transcription output before cleanup\n",
    "    input(\"Press Enter to stop...\")\n",
    "    \n",
    "    # Gracefully stop the transcription process\n",
    "    # This ensures proper cleanup of resources and connection closure\n",
    "    transcriber.transcriber.stop_transcribing_async().get()\n",
    "    \n",
    "call_transcriber()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
