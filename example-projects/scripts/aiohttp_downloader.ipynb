{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cc10102",
   "metadata": {},
   "source": [
    "# Robust Async File Downloader in Python with aiohttp\n",
    "---\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Efficiently downloading large files in Python can be challenging, especially when you want to support asynchronous downloads, caching, file validation, and real-time progress feedback. In this blog, we'll walk through building a production-ready async file downloader using the `aiohttp` library, with features like cache validation, file size and MD5 hash checking, and a customizable progress callback.\n",
    "\n",
    "\n",
    "# Why Choose aiohttp for Asynchronous Downloads?\n",
    "\n",
    "`aiohttp` is my go-to library for asynchronous HTTP operations in Python. It’s fast, mature, and designed for non-blocking network tasks—making it ideal for:\n",
    "\n",
    "- Downloading large files efficiently\n",
    "- Handling multiple downloads concurrently\n",
    "- Integrating with modern async Python workflows\n",
    "\n",
    "\n",
    "# Key Features of Our Async File Downloader\n",
    "\n",
    "Here’s what sets this downloader apart:\n",
    "\n",
    "- **Asynchronous Downloading**: Harness the power of async/await for non-blocking file transfers.\n",
    "- **Smart Caching**: Skip downloads if the file already exists and matches expected size or MD5 hash.\n",
    "- **Robust Validation**: Automatically check file size and MD5 hash after download to ensure integrity.\n",
    "- **Custom Progress Callback**: Get real-time feedback with a callback function for download progress.\n",
    "\n",
    "\n",
    "# Implementation Overview\n",
    "\n",
    "Below is a streamlined version of the `AsyncFileDownloader` class. It’s designed for clarity and extensibility:\n",
    "\n",
    "```python\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "class AsyncFileDownloader:\n",
    "    def __init__(self, output_dir=\".\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    async def _md5sum(self, file_path, chunk_size=8192):\n",
    "        md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            while True:\n",
    "                chunk = f.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                md5.update(chunk)\n",
    "        return md5.hexdigest()\n",
    "\n",
    "    async def _validate_file(self, file_path, expected_size=None, expected_md5=None):\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        if expected_size is not None and os.path.getsize(file_path) != expected_size:\n",
    "            return False\n",
    "        if expected_md5 is not None:\n",
    "            actual_md5 = await self._md5sum(file_path)\n",
    "            if actual_md5 != expected_md5:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    async def download(self, url, filename=None, expected_size=None, expected_md5=None, callback=None, frequency=0.5):\n",
    "        if not filename:\n",
    "            filename = os.path.basename(url)\n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "\n",
    "        # Cache validation\n",
    "        if await self._validate_file(file_path, expected_size, expected_md5):\n",
    "            print(f\"File {file_path} already valid. Skipping download.\")\n",
    "            return file_path\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                total_bytes = int(resp.headers.get('Content-Length', 0)) or expected_size or 0\n",
    "                bytes_downloaded = 0\n",
    "                start_time = time.time()\n",
    "                last_callback = start_time\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    async for chunk in resp.content.iter_chunked(8192):\n",
    "                        f.write(chunk)\n",
    "                        bytes_downloaded += len(chunk)\n",
    "                        now = time.time()\n",
    "                        if callback and (now - last_callback >= frequency or bytes_downloaded == total_bytes):\n",
    "                            time_elapsed = now - start_time\n",
    "                            callback(bytes_downloaded, total_bytes, time_elapsed)\n",
    "                            last_callback = now\n",
    "        print(f\"Downloaded {file_path}\")\n",
    "\n",
    "        # Post-download validation\n",
    "        if not await self._validate_file(file_path, expected_size, expected_md5):\n",
    "            os.remove(file_path)\n",
    "            raise ValueError(f\"Downloaded file {file_path} failed validation.\")\n",
    "        return file_path\n",
    "```\n",
    "\n",
    "\n",
    "Let’s break down the workflow:\n",
    "\n",
    "1. **Initialization**: Set your output directory for downloads.\n",
    "2. **Cache Validation**: Before downloading, check if the file already exists and matches the expected size or MD5 hash.\n",
    "3. **Async Download**: If needed, stream the file in chunks and write to disk.\n",
    "4. **Progress Callback**: Receive real-time updates on download progress, bytes transferred, and elapsed time.\n",
    "5. **Post-download Validation**: After download, validate the file again. If it fails, delete and raise an error.\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "\n",
    "Here’s how you can use the downloader in your own projects:\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "\n",
    "def print_progress(bytes_downloaded, total_bytes, time_elapsed):\n",
    "    percent = (bytes_downloaded / total_bytes) * 100 if total_bytes else 0\n",
    "    print(f\"Downloaded: {bytes_downloaded}/{total_bytes} bytes ({percent:.2f}%), Time elapsed: {time_elapsed:.2f}s\")\n",
    "\n",
    "async def main():\n",
    "    downloader = AsyncFileDownloader(output_dir=\"downloads\")\n",
    "    await downloader.download(\n",
    "        url=\"https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles1.xml-p1p41242.bz2\",\n",
    "        expected_size=291274499,\n",
    "        expected_md5=\"5b594c2af71ecf65505dc42d49ab6121\",\n",
    "        callback=print_progress,\n",
    "        frequency=1.0\n",
    "    )\n",
    "\n",
    "asyncio.run(main())\n",
    "```\n",
    "\n",
    "\n",
    "# Other Considerations\n",
    "\n",
    "- **Limit Concurrency**: For large files or many simultaneous downloads, use a semaphore or queue to avoid overwhelming your system.\n",
    "- **Validate Everything**: Always check files after download to guarantee data integrity.\n",
    "- **Explore Alternatives**: While `aiohttp` is excellent, consider `httpx` for advanced async HTTP needs.\n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Building a robust async file downloader in Python is easier than you might think! With `aiohttp`, you get speed, reliability, and flexibility—perfect for data engineering, web scraping, and AI workflows.\n",
    "\n",
    "If you found this post helpful, consider subscribing to my newsletter for more deep dives into Python, AI, and engineering best practices. \n",
    "\n",
    "Have questions, feedback, or your own download tips? Drop a comment below\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9883cf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Async File Downloader in Python using aiohttp\n",
    "\n",
    "This module provides a class AsyncFileDownloader for downloading files asynchronously using aiohttp. It supports optional file size and md5 hash validation, and can skip downloads if the file in the output directory already matches the expected size and/or md5 hash.\n",
    "\"\"\"\n",
    "\n",
    "import aiohttp\n",
    "import asyncio\n",
    "import hashlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "class AsyncFileDownloader:\n",
    "    def __init__(self, output_dir=\".\"):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    async def _md5sum(self, file_path, chunk_size=8192):\n",
    "        md5 = hashlib.md5()\n",
    "        try:\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                while True:\n",
    "                    chunk = f.read(chunk_size)\n",
    "                    if not chunk:\n",
    "                        break\n",
    "                    md5.update(chunk)\n",
    "            return md5.hexdigest()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    async def _validate_file(self, file_path, expected_size=None, expected_md5=None):\n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        if expected_size is not None:\n",
    "            if os.path.getsize(file_path) != expected_size:\n",
    "                return False\n",
    "        if expected_md5 is not None:\n",
    "            actual_md5 = await self._md5sum(file_path)\n",
    "            if actual_md5 != expected_md5:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    async def download(self, url, filename=None, expected_size=None, expected_md5=None, callback=None, frequency=None):\n",
    "        if not filename:\n",
    "            filename = os.path.basename(url)\n",
    "        file_path = os.path.join(self.output_dir, filename)\n",
    "\n",
    "        # Cache validation\n",
    "        if await self._validate_file(file_path, expected_size, expected_md5):\n",
    "            print(f\"File {file_path} already valid. Skipping download.\")\n",
    "            return file_path\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as resp:\n",
    "                resp.raise_for_status()\n",
    "                total_bytes = int(resp.headers.get('Content-Length', 0)) or expected_size or 0\n",
    "                bytes_downloaded = 0\n",
    "                start_time = time.time()\n",
    "                last_callback = start_time\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    async for chunk in resp.content.iter_chunked(8192):\n",
    "                        f.write(chunk)\n",
    "                        bytes_downloaded += len(chunk)\n",
    "                        now = time.time()\n",
    "                        \n",
    "                        if callback and frequency and (now - last_callback >= frequency or bytes_downloaded == total_bytes):\n",
    "                            time_elapsed = now - start_time\n",
    "                            callback(bytes_downloaded, total_bytes, time_elapsed)\n",
    "                            last_callback = now\n",
    "        print(f\"Downloaded {file_path}\")\n",
    "\n",
    "        # Post-download validation\n",
    "        if not await self._validate_file(file_path, expected_size, expected_md5):\n",
    "            os.remove(file_path)\n",
    "            raise ValueError(f\"Downloaded file {file_path} failed validation.\")\n",
    "        return file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4ab9bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloads/enwiki-20250920-pages-articles1.xml-p1p41242.bz2 already valid. Skipping download.\n"
     ]
    }
   ],
   "source": [
    "#Example usage:\n",
    "url='https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2-rss.xml'\n",
    "url='https://dumps.wikimedia.org/enwiki/20250920/enwiki-20250920-pages-articles1.xml-p1p41242.bz2'\n",
    "md5=\"5b594c2af71ecf65505dc42d49ab6121\"\n",
    "size=291274499\n",
    "\n",
    "def print_progress(bytes_downloaded, total_bytes, time_elapsed):\n",
    "    percent = (bytes_downloaded / total_bytes) * 100\n",
    "    print(f\"Downloaded: {bytes_downloaded}/{total_bytes} bytes \"\n",
    "          f\"({percent:.2f}%), Time elapsed: {time_elapsed:.2f}s\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    downloader = AsyncFileDownloader(output_dir=\"downloads\")\n",
    "    await downloader.download(\n",
    "        url=url,\n",
    "        expected_size=size,\n",
    "        expected_md5=md5,callback=print_progress, frequency=1\n",
    "    )\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
